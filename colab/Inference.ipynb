{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E10H6m8VONy2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import math\n",
        "import PIL\n",
        "import numbers\n",
        "import cv2\n",
        "\n",
        "\n",
        "def add_text_to_image(\n",
        "    img,\n",
        "    text,\n",
        "    font=cv2.FONT_ITALIC,\n",
        "    bottomLeftCornerOfText=(10, 20),\n",
        "    fontScale=0.4,\n",
        "    fontColor=(200, 200, 200),\n",
        "    lineType=1,\n",
        "):\n",
        "\n",
        "    color = np.random.randint(0, 255, size=(3,))\n",
        "    color = (int(color[0]), int(color[1]), int(color[2]))\n",
        "    img_with_text = cv2.putText(\n",
        "        img, text, bottomLeftCornerOfText, font, fontScale, color, lineType\n",
        "    )\n",
        "    return img_with_text\n",
        "\n",
        "\n",
        "def resize_clip(clip, size, interpolation=\"bilinear\"):\n",
        "    if isinstance(clip[0], np.ndarray):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_h, im_w, im_c = clip[0].shape\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (\n",
        "                im_h <= im_w and im_h == size\n",
        "            ):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "        if interpolation == \"bilinear\":\n",
        "            np_inter = cv2.INTER_LINEAR\n",
        "        else:\n",
        "            np_inter = cv2.INTER_NEAREST\n",
        "        scaled = [cv2.resize(img, size, interpolation=np_inter) for img in clip]\n",
        "    elif isinstance(clip[0], PIL.Image.Image):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_w, im_h = clip[0].size\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (\n",
        "                im_h <= im_w and im_h == size\n",
        "            ):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "        if interpolation == \"bilinear\":\n",
        "            pil_inter = PIL.Image.BILINEAR\n",
        "        else:\n",
        "            pil_inter = PIL.Image.NEAREST\n",
        "        scaled = [img.resize(size, pil_inter) for img in clip]\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            \"Expected numpy.ndarray or PIL.Image\"\n",
        "            + \"but got list of {0}\".format(type(clip[0]))\n",
        "        )\n",
        "    return np.asarray(scaled)\n",
        "\n",
        "\n",
        "def get_resize_sizes(im_h, im_w, size):\n",
        "    if im_w < im_h:\n",
        "        ow = size\n",
        "        oh = int(size * im_h / im_w)\n",
        "    else:\n",
        "        oh = size\n",
        "        ow = int(size * im_w / im_h)\n",
        "    return oh, ow\n",
        "\n",
        "\n",
        "def normalize_color_input_zero_center_unit_range(frames, max_val=255.0):\n",
        "\n",
        "    frames = (frames / max_val) * 2 - 1\n",
        "    return frames\n",
        "\n",
        "\n",
        "class normalizeColorInputZeroCenterUnitRange(object):\n",
        "    def __init__(self, max_val=255.0):\n",
        "\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        result = normalize_color_input_zero_center_unit_range(\n",
        "            input_tensor, max_val=self.max_val\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "def random_select(frames, n, seed=None):\n",
        "    \"\"\"\n",
        "    Takes multiple frames as ndarray with shape\n",
        "    (frame id, height, width, channels) and selects\n",
        "    randomly n-frames. If n is greater than the number\n",
        "    of overall frames, placeholder frames (zeros) will\n",
        "    be added.\n",
        "\n",
        "    frames: numpy\n",
        "        all frames (e.g. video) with shape\n",
        "        (frame id, height, width, channels)\n",
        "    n: int\n",
        "        number of desired randomly picked frames\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy: frames\n",
        "        randomly picked frames with shape\n",
        "        (frame id, height, width, channels)\n",
        "    \"\"\"\n",
        "    # print(\"Frames shape:{}\".format(np.shape(frames)))\n",
        "    if seed is not None:\n",
        "        seed(seed)\n",
        "\n",
        "    number_of_frames = np.shape(frames)[0]\n",
        "    if number_of_frames < n:\n",
        "        # Add all frames\n",
        "        selected_frames = []\n",
        "        for i in range(number_of_frames):\n",
        "            frame = frames[i, :, :, :]\n",
        "            selected_frames.append(frame)\n",
        "\n",
        "        # Fill up with 'placeholder' images\n",
        "        frame = np.zeros(frames[0, :, :, :].shape)\n",
        "        for i in range(n - number_of_frames):\n",
        "            selected_frames.append(frame)\n",
        "\n",
        "        return np.array(selected_frames)\n",
        "\n",
        "    # Selected random frame ids\n",
        "    frame_ids = set([])\n",
        "    while len(frame_ids) < n:\n",
        "        frame_ids.add(randint(0, number_of_frames - 1))\n",
        "\n",
        "    # Sort the frame ids\n",
        "    frame_ids = sorted(frame_ids)\n",
        "\n",
        "    # Select frames\n",
        "    selected_frames = []\n",
        "    for id in frame_ids:\n",
        "        # print (np.shape(frames))\n",
        "\n",
        "        frame = frames[id, :, :, :]\n",
        "        selected_frames.append(frame)\n",
        "\n",
        "    return np.array(selected_frames)\n",
        "\n",
        "\n",
        "def center_crop(frames, height, width, pad_zeros_if_too_small=True):\n",
        "    \"\"\"\n",
        "    Takes multiple frames as ndarray with shape\n",
        "    (frame id, height, width, channels) and crops all\n",
        "    frames centered to desired width and height.\n",
        "\n",
        "    frames: numpy\n",
        "        all frames (e.g. video) with shape\n",
        "        (frame id, height, width, channels)\n",
        "    height: int\n",
        "        height of the resulting crop\n",
        "    width: int\n",
        "        width of the resulting crop\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy: frames\n",
        "        centered cropped frames with shape\n",
        "        (frame id, height, width, channels)\n",
        "    \"\"\"\n",
        "\n",
        "    frame_height = np.shape(frames)[1]\n",
        "    frame_width = np.shape(frames)[2]\n",
        "\n",
        "    t = np.shape(frames)[0]\n",
        "    channels = np.shape(frames)[3]\n",
        "\n",
        "    if pad_zeros_if_too_small and (\n",
        "        height > frame_height or width > frame_width\n",
        "    ):\n",
        "        # desired width\n",
        "        frames_new = np.zeros(\n",
        "            (t, max(frame_height, height), max(frame_width, width), channels)\n",
        "        )\n",
        "        # fill with the old data\n",
        "        frames_new[0:t, 0:frame_height, 0:frame_width, 0:channels] = frames\n",
        "        frames = frames_new\n",
        "        frame_height = np.shape(frames)[1]\n",
        "        frame_width = np.shape(frames)[2]\n",
        "\n",
        "    origin_x = (frame_width - width) / 2\n",
        "    origin_y = (frame_height - height) / 2\n",
        "\n",
        "    # Floor origin (miss matching input sizes)\n",
        "    # E.g. input width of 171 and crop width 112\n",
        "    # would result in a float.\n",
        "    origin_x = math.floor(origin_x)\n",
        "    origin_y = math.floor(origin_y)\n",
        "\n",
        "    return frames[\n",
        "        :, origin_y : origin_y + height, origin_x : origin_x + width, :\n",
        "    ]\n",
        "\n",
        "\n",
        "class Rescale(object):\n",
        "    def __init__(self, size, interpolation=\"bilinear\"):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, clip):\n",
        "\n",
        "        resized = resize_clip(clip, self.size, interpolation=self.interpolation)\n",
        "        return resized\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "        result = center_crop(input_tensor, self.height, self.width)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "        result = random_select(input_tensor, self.n)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "        # Swap color channels axis because\n",
        "        # numpy frames: Frames ID x Height x Width x Channels\n",
        "        # torch frames: Channels x Frame ID x Height x Width\n",
        "\n",
        "        result = input_tensor.transpose(3, 0, 1, 2)\n",
        "        result = np.float32(result)\n",
        "\n",
        "        return torch.from_numpy(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oLYU3bJCOHNw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import collections.abc as container_abcs\n",
        "import operator\n",
        "import torch\n",
        "\n",
        "\n",
        "class ModuleList(torch.nn.Module):\n",
        "    r\"\"\"Holds submodules in a list.\n",
        "\n",
        "        ModuleList can be indexed like a regular Python list, but modules it\n",
        "        contains are properly registered, and will be visible by all Module methods.\n",
        "\n",
        "        Arguments:\n",
        "            modules (iterable, optional): an iterable of modules to add\n",
        "    df\n",
        "        Example::\n",
        "\n",
        "            class MyModule(nn.Module):\n",
        "                def __init__(self):\n",
        "                    super(MyModule, self).__init__()\n",
        "                    self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
        "\n",
        "                def forward(self, x):\n",
        "                    # ModuleList can act as an iterable, or be indexed using ints\n",
        "                    for i, l in enumerate(self.linears):\n",
        "                        x = self.linears[i // 2](x) + l(x)\n",
        "                    return x\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modules=None):\n",
        "        super(ModuleList, self).__init__()\n",
        "        if modules is not None:\n",
        "            self += modules\n",
        "\n",
        "    def _get_abs_string_index(self, idx):\n",
        "        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
        "        idx = operator.index(idx)\n",
        "        if not (-len(self) <= idx < len(self)):\n",
        "            raise IndexError(\"index {} is out of range\".format(idx))\n",
        "        if idx < 0:\n",
        "            idx += len(self)\n",
        "        return str(idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            return self.__class__(list(self._modules.values())[idx])\n",
        "        else:\n",
        "            return self._modules[self._get_abs_string_index(idx)]\n",
        "\n",
        "    def __setitem__(self, idx, module):\n",
        "        idx = self._get_abs_string_index(idx)\n",
        "        return setattr(self, str(idx), module)\n",
        "\n",
        "    def __delitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            for k in range(len(self._modules))[idx]:\n",
        "                delattr(self, str(k))\n",
        "        else:\n",
        "            delattr(self, self._get_abs_string_index(idx))\n",
        "        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n",
        "        str_indices = [str(i) for i in range(len(self._modules))]\n",
        "        self._modules = OrderedDict(\n",
        "            list(zip(str_indices, self._modules.values()))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._modules)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self._modules.values())\n",
        "\n",
        "    def __iadd__(self, modules):\n",
        "        return self.extend(modules)\n",
        "\n",
        "    def __dir__(self):\n",
        "        keys = super(ModuleList, self).__dir__()\n",
        "        keys = [key for key in keys if not key.isdigit()]\n",
        "        return keys\n",
        "\n",
        "    def insert(self, index, module):\n",
        "        r\"\"\"Insert a given module before a given index in the list.\n",
        "\n",
        "        Arguments:\n",
        "            index (int): index to insert.\n",
        "            module (nn.Module): module to insert\n",
        "        \"\"\"\n",
        "        for i in range(len(self._modules), index, -1):\n",
        "            self._modules[str(i)] = self._modules[str(i - 1)]\n",
        "        self._modules[str(index)] = module\n",
        "\n",
        "    def append(self, module):\n",
        "        r\"\"\"Appends a given module to the end of the list.\n",
        "\n",
        "        Arguments:\n",
        "            module (nn.Module): module to append\n",
        "        \"\"\"\n",
        "        self.add_module(str(len(self)), module)\n",
        "        return self\n",
        "\n",
        "    def extend(self, modules):\n",
        "        r\"\"\"Appends modules from a Python iterable to the end of the list.\n",
        "\n",
        "        Arguments:\n",
        "            modules (iterable): iterable of modules to append\n",
        "        \"\"\"\n",
        "        if not isinstance(modules, container_abcs.Iterable):\n",
        "            raise TypeError(\n",
        "                \"ModuleList.extend should be called with an \"\n",
        "                \"iterable, but got \" + type(modules).__name__\n",
        "            )\n",
        "        offset = len(self)\n",
        "        for i, module in enumerate(modules):\n",
        "            self.add_module(str(offset + i), module)\n",
        "        return self\n",
        "\n",
        "\n",
        "def get_padding_shape(filter_shape, stride):\n",
        "    def _pad_top_bottom(filter_dim, stride_val):\n",
        "        pad_along = max(filter_dim - stride_val, 0)\n",
        "        pad_top = pad_along // 2\n",
        "        pad_bottom = pad_along - pad_top\n",
        "        return pad_top, pad_bottom\n",
        "\n",
        "    padding_shape = []\n",
        "    for filter_dim, stride_val in zip(filter_shape, stride):\n",
        "        pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)\n",
        "        padding_shape.append(pad_top)\n",
        "        padding_shape.append(pad_bottom)\n",
        "    depth_top = padding_shape.pop(0)\n",
        "    depth_bottom = padding_shape.pop(0)\n",
        "    padding_shape.append(depth_top)\n",
        "    padding_shape.append(depth_bottom)\n",
        "\n",
        "    return tuple(padding_shape)\n",
        "\n",
        "\n",
        "def simplify_padding(padding_shapes):\n",
        "    all_same = True\n",
        "    padding_init = padding_shapes[0]\n",
        "    for pad in padding_shapes[1:]:\n",
        "        if pad != padding_init:\n",
        "            all_same = False\n",
        "    return all_same, padding_init\n",
        "\n",
        "\n",
        "class Unit3Dpy(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=(1, 1, 1),\n",
        "        stride=(1, 1, 1),\n",
        "        activation=\"relu\",\n",
        "        padding=\"SAME\",\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "    ):\n",
        "        super(Unit3Dpy, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.use_bn = use_bn\n",
        "        if padding == \"SAME\":\n",
        "            padding_shape = get_padding_shape(kernel_size, stride)\n",
        "            simplify_pad, pad_size = simplify_padding(padding_shape)\n",
        "            self.simplify_pad = simplify_pad\n",
        "        elif padding == \"VALID\":\n",
        "            padding_shape = 0\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"padding should be in [VALID|SAME] but got {}\".format(padding)\n",
        "            )\n",
        "\n",
        "        if padding == \"SAME\":\n",
        "            if not simplify_pad:\n",
        "                self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n",
        "                self.conv3d = torch.nn.Conv3d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size,\n",
        "                    stride=stride,\n",
        "                    bias=use_bias,\n",
        "                )\n",
        "            else:\n",
        "                self.conv3d = torch.nn.Conv3d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=pad_size,\n",
        "                    bias=use_bias,\n",
        "                )\n",
        "        elif padding == \"VALID\":\n",
        "            self.conv3d = torch.nn.Conv3d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                padding=padding_shape,\n",
        "                stride=stride,\n",
        "                bias=use_bias,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"padding should be in [VALID|SAME] but got {}\".format(padding)\n",
        "            )\n",
        "\n",
        "        if self.use_bn:\n",
        "            self.batch3d = torch.nn.BatchNorm3d(out_channels)\n",
        "\n",
        "        if activation == \"relu\":\n",
        "            self.activation = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if self.padding == \"SAME\" and self.simplify_pad is False:\n",
        "            inp = self.pad(inp)\n",
        "        out = self.conv3d(inp)\n",
        "        if self.use_bn:\n",
        "            out = self.batch3d(out)\n",
        "        if self.activation is not None:\n",
        "            out = torch.nn.functional.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MaxPool3dTFPadding(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, stride=None, padding=\"SAME\"):\n",
        "        super(MaxPool3dTFPadding, self).__init__()\n",
        "        if padding == \"SAME\":\n",
        "            padding_shape = get_padding_shape(kernel_size, stride)\n",
        "            self.padding_shape = padding_shape\n",
        "            self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n",
        "        self.pool = torch.nn.MaxPool3d(kernel_size, stride, ceil_mode=True)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = self.pad(inp)\n",
        "        out = self.pool(inp)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Mixed, self).__init__()\n",
        "        # Branch 0\n",
        "        self.branch_0 = Unit3Dpy(\n",
        "            in_channels, out_channels[0], kernel_size=(1, 1, 1)\n",
        "        )\n",
        "\n",
        "        # Branch 1\n",
        "        branch_1_conv1 = Unit3Dpy(\n",
        "            in_channels, out_channels[1], kernel_size=(1, 1, 1)\n",
        "        )\n",
        "        branch_1_conv2 = Unit3Dpy(\n",
        "            out_channels[1], out_channels[2], kernel_size=(3, 3, 3)\n",
        "        )\n",
        "        self.branch_1 = torch.nn.Sequential(branch_1_conv1, branch_1_conv2)\n",
        "\n",
        "        # Branch 2\n",
        "        branch_2_conv1 = Unit3Dpy(\n",
        "            in_channels, out_channels[3], kernel_size=(1, 1, 1)\n",
        "        )\n",
        "        branch_2_conv2 = Unit3Dpy(\n",
        "            out_channels[3], out_channels[4], kernel_size=(3, 3, 3)\n",
        "        )\n",
        "        self.branch_2 = torch.nn.Sequential(branch_2_conv1, branch_2_conv2)\n",
        "\n",
        "        # Branch3\n",
        "        branch_3_pool = MaxPool3dTFPadding(\n",
        "            kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=\"SAME\"\n",
        "        )\n",
        "        branch_3_conv2 = Unit3Dpy(\n",
        "            in_channels, out_channels[5], kernel_size=(1, 1, 1)\n",
        "        )\n",
        "        self.branch_3 = torch.nn.Sequential(branch_3_pool, branch_3_conv2)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out_0 = self.branch_0(inp)\n",
        "        out_1 = self.branch_1(inp)\n",
        "        out_2 = self.branch_2(inp)\n",
        "        out_3 = self.branch_3(inp)\n",
        "        out = torch.cat((out_0, out_1, out_2, out_3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class I3D(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, num_classes, modality=\"rgb\", dropout_prob=0, name=\"inception\"\n",
        "    ):\n",
        "        super(I3D, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.num_classes = num_classes\n",
        "        if modality == \"rgb\":\n",
        "            in_channels = 3\n",
        "        elif modality == \"flow\":\n",
        "            in_channels = 2\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"{} not among known modalities [rgb|flow]\".format(modality)\n",
        "            )\n",
        "        self.modality = modality\n",
        "\n",
        "        conv3d_1a_7x7 = Unit3Dpy(\n",
        "            out_channels=64,\n",
        "            in_channels=in_channels,\n",
        "            kernel_size=(7, 7, 7),\n",
        "            stride=(2, 2, 2),\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        # 1st conv-pool\n",
        "        self.conv3d_1a_7x7 = conv3d_1a_7x7\n",
        "        self.maxPool3d_2a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=\"SAME\"\n",
        "        )\n",
        "        # conv conv\n",
        "        conv3d_2b_1x1 = Unit3Dpy(\n",
        "            out_channels=64,\n",
        "            in_channels=64,\n",
        "            kernel_size=(1, 1, 1),\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        self.conv3d_2b_1x1 = conv3d_2b_1x1\n",
        "        conv3d_2c_3x3 = Unit3Dpy(\n",
        "            out_channels=192,\n",
        "            in_channels=64,\n",
        "            kernel_size=(3, 3, 3),\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        self.conv3d_2c_3x3 = conv3d_2c_3x3\n",
        "        self.maxPool3d_3a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        # Mixed_3b\n",
        "        self.mixed_3b = Mixed(192, [64, 96, 128, 16, 32, 32])\n",
        "        self.mixed_3c = Mixed(256, [128, 128, 192, 32, 96, 64])\n",
        "\n",
        "        self.maxPool3d_4a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        # Mixed 4\n",
        "        self.mixed_4b = Mixed(480, [192, 96, 208, 16, 48, 64])\n",
        "        self.mixed_4c = Mixed(512, [160, 112, 224, 24, 64, 64])\n",
        "        self.mixed_4d = Mixed(512, [128, 128, 256, 24, 64, 64])\n",
        "        self.mixed_4e = Mixed(512, [112, 144, 288, 32, 64, 64])\n",
        "        self.mixed_4f = Mixed(528, [256, 160, 320, 32, 128, 128])\n",
        "\n",
        "        self.maxPool3d_5a_2x2 = MaxPool3dTFPadding(\n",
        "            kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=\"SAME\"\n",
        "        )\n",
        "\n",
        "        # Mixed 5\n",
        "        self.mixed_5b = Mixed(832, [256, 160, 320, 32, 128, 128])\n",
        "        self.mixed_5c = Mixed(832, [384, 192, 384, 48, 128, 128])\n",
        "\n",
        "        self.avg_pool = torch.nn.AvgPool3d((2, 7, 7), (1, 1, 1))\n",
        "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "        self.conv3d_0c_1x1 = Unit3Dpy(\n",
        "            in_channels=1024,\n",
        "            out_channels=self.num_classes,\n",
        "            kernel_size=(1, 1, 1),\n",
        "            activation=None,\n",
        "            use_bias=True,\n",
        "            use_bn=False,\n",
        "        )\n",
        "        self.softmax = torch.nn.Softmax(1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Preprocessing\n",
        "        # print(\"Input shape: {}\".format(inp.size()))\n",
        "        out = self.conv3d_1a_7x7(inp)\n",
        "\n",
        "        # print(\"Shape after out = self.conv3d_1a_7x7(inp): {}\".format(out.size()))\n",
        "        out = self.maxPool3d_2a_3x3(out)\n",
        "        out = self.conv3d_2b_1x1(out)\n",
        "        out = self.conv3d_2c_3x3(out)\n",
        "        out = self.maxPool3d_3a_3x3(out)\n",
        "        out = self.mixed_3b(out)\n",
        "        out = self.mixed_3c(out)\n",
        "        out = self.maxPool3d_4a_3x3(out)\n",
        "        # print(\"Shape after out = self.maxPool3d_4a_3x3(inp): {}\".format(out.size()))\n",
        "        out = self.mixed_4b(out)\n",
        "        out = self.mixed_4c(out)\n",
        "        out = self.mixed_4d(out)\n",
        "        out = self.mixed_4e(out)\n",
        "        out = self.mixed_4f(out)\n",
        "        out = self.maxPool3d_5a_2x2(out)\n",
        "        out = self.mixed_5b(out)\n",
        "        out = self.mixed_5c(out)\n",
        "        # print(\"Shape after out = self.mixed_5c(out): {}\".format(out.size()))\n",
        "        out = self.avg_pool(out)\n",
        "        # print(\"Shape after self.avg_pool(out: {}\".format(out.size()))\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv3d_0c_1x1(out)\n",
        "        #\n",
        "        # print(\"Shape after out = self.conv3d_0c_1x1(out)): {}\".format(out.size()))\n",
        "\n",
        "        out = out.squeeze(3)\n",
        "        out = out.squeeze(3)\n",
        "        # print (\"Shaper after out = out.squeeze(3): {}\".format(out.size()))\n",
        "        out = out.mean(2)\n",
        "        # print(\"Shaper after out = out.mean(2): {}\".format(out.size()))\n",
        "\n",
        "        out_logits = out\n",
        "        out = self.softmax(out_logits)\n",
        "\n",
        "        # return out, out_logits\n",
        "        return out\n",
        "\n",
        "    def load_tf_weights(self, sess):\n",
        "        state_dict = {}\n",
        "        if self.modality == \"rgb\":\n",
        "            prefix = \"RGB/inception_i3d\"\n",
        "        elif self.modality == \"flow\":\n",
        "            prefix = \"Flow/inception_i3d\"\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            \"conv3d_1a_7x7\",\n",
        "            sess,\n",
        "            os.path.join(prefix, \"Conv3d_1a_7x7\"),\n",
        "        )\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            \"conv3d_2b_1x1\",\n",
        "            sess,\n",
        "            os.path.join(prefix, \"Conv3d_2b_1x1\"),\n",
        "        )\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            \"conv3d_2c_3x3\",\n",
        "            sess,\n",
        "            os.path.join(prefix, \"Conv3d_2c_3x3\"),\n",
        "        )\n",
        "\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_3b\", sess, os.path.join(prefix, \"Mixed_3b\")\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_3c\", sess, os.path.join(prefix, \"Mixed_3c\")\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_4b\", sess, os.path.join(prefix, \"Mixed_4b\")\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_4c\", sess, os.path.join(prefix, \"Mixed_4c\")\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_4d\", sess, os.path.join(prefix, \"Mixed_4d\")\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_4e\", sess, os.path.join(prefix, \"Mixed_4e\")\n",
        "        )\n",
        "        # Here goest to 0.1 max error with tf\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_4f\", sess, os.path.join(prefix, \"Mixed_4f\")\n",
        "        )\n",
        "\n",
        "        load_mixed(\n",
        "            state_dict,\n",
        "            \"mixed_5b\",\n",
        "            sess,\n",
        "            os.path.join(prefix, \"Mixed_5b\"),\n",
        "            fix_typo=True,\n",
        "        )\n",
        "        load_mixed(\n",
        "            state_dict, \"mixed_5c\", sess, os.path.join(prefix, \"Mixed_5c\")\n",
        "        )\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            \"conv3d_0c_1x1\",\n",
        "            sess,\n",
        "            os.path.join(prefix, \"Logits\", \"Conv3d_0c_1x1\"),\n",
        "            bias=True,\n",
        "            bn=False,\n",
        "        )\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def get_conv_params(sess, name, bias=False):\n",
        "    # Get conv weights\n",
        "    conv_weights_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, \"w:0\")\n",
        "    )\n",
        "    if bias:\n",
        "        conv_bias_tensor = sess.graph.get_tensor_by_name(\n",
        "            os.path.join(name, \"b:0\")\n",
        "        )\n",
        "        conv_bias = sess.run(conv_bias_tensor)\n",
        "    conv_weights = sess.run(conv_weights_tensor)\n",
        "    conv_shape = conv_weights.shape\n",
        "\n",
        "    kernel_shape = conv_shape[0:3]\n",
        "    in_channels = conv_shape[3]\n",
        "    out_channels = conv_shape[4]\n",
        "\n",
        "    conv_op = sess.graph.get_operation_by_name(\n",
        "        os.path.join(name, \"convolution\")\n",
        "    )\n",
        "    padding_name = conv_op.get_attr(\"padding\")\n",
        "    padding = _get_padding(padding_name, kernel_shape)\n",
        "    all_strides = conv_op.get_attr(\"strides\")\n",
        "    strides = all_strides[1:4]\n",
        "    conv_params = [\n",
        "        conv_weights,\n",
        "        kernel_shape,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides,\n",
        "        padding,\n",
        "    ]\n",
        "    if bias:\n",
        "        conv_params.append(conv_bias)\n",
        "    return conv_params\n",
        "\n",
        "\n",
        "def get_bn_params(sess, name):\n",
        "    moving_mean_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, \"moving_mean:0\")\n",
        "    )\n",
        "    moving_var_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, \"moving_variance:0\")\n",
        "    )\n",
        "    beta_tensor = sess.graph.get_tensor_by_name(os.path.join(name, \"beta:0\"))\n",
        "    moving_mean = sess.run(moving_mean_tensor)\n",
        "    moving_var = sess.run(moving_var_tensor)\n",
        "    beta = sess.run(beta_tensor)\n",
        "    return moving_mean, moving_var, beta\n",
        "\n",
        "\n",
        "def _get_padding(padding_name, conv_shape):\n",
        "    padding_name = padding_name.decode(\"utf-8\")\n",
        "    if padding_name == \"VALID\":\n",
        "        return [0, 0]\n",
        "    elif padding_name == \"SAME\":\n",
        "        return [\n",
        "            math.floor(int(conv_shape[0]) / 2),\n",
        "            math.floor(int(conv_shape[1]) / 2),\n",
        "            math.floor(int(conv_shape[2]) / 2),\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid padding name \" + padding_name)\n",
        "\n",
        "\n",
        "def load_conv3d(state_dict, name_pt, sess, name_tf, bias=False, bn=True):\n",
        "    # Transfer convolution params\n",
        "    conv_name_tf = os.path.join(name_tf, \"conv_3d\")\n",
        "    conv_params = get_conv_params(sess, conv_name_tf, bias=bias)\n",
        "    if bias:\n",
        "        (\n",
        "            conv_weights,\n",
        "            kernel_shape,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            strides,\n",
        "            padding,\n",
        "            conv_bias,\n",
        "        ) = conv_params\n",
        "    else:\n",
        "        (\n",
        "            conv_weights,\n",
        "            kernel_shape,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            strides,\n",
        "            padding,\n",
        "        ) = conv_params\n",
        "\n",
        "    conv_weights_rs = np.transpose(\n",
        "        conv_weights, (4, 3, 0, 1, 2)\n",
        "    )  # to pt format (out_c, in_c, depth, height, width)\n",
        "    state_dict[name_pt + \".conv3d.weight\"] = torch.from_numpy(conv_weights_rs)\n",
        "    if bias:\n",
        "        state_dict[name_pt + \".conv3d.bias\"] = torch.from_numpy(conv_bias)\n",
        "\n",
        "    # Transfer batch norm params\n",
        "    if bn:\n",
        "        conv_tf_name = os.path.join(name_tf, \"batch_norm\")\n",
        "        moving_mean, moving_var, beta = get_bn_params(sess, conv_tf_name)\n",
        "\n",
        "        out_planes = conv_weights_rs.shape[0]\n",
        "        state_dict[name_pt + \".batch3d.weight\"] = torch.ones(out_planes)\n",
        "        state_dict[name_pt + \".batch3d.bias\"] = torch.from_numpy(beta)\n",
        "        state_dict[name_pt + \".batch3d.running_mean\"] = torch.from_numpy(\n",
        "            moving_mean\n",
        "        )\n",
        "        state_dict[name_pt + \".batch3d.running_var\"] = torch.from_numpy(\n",
        "            moving_var\n",
        "        )\n",
        "\n",
        "\n",
        "def load_mixed(state_dict, name_pt, sess, name_tf, fix_typo=False):\n",
        "    # Branch 0\n",
        "    load_conv3d(\n",
        "        state_dict,\n",
        "        name_pt + \".branch_0\",\n",
        "        sess,\n",
        "        os.path.join(name_tf, \"Branch_0/Conv3d_0a_1x1\"),\n",
        "    )\n",
        "\n",
        "    # Branch .1\n",
        "    load_conv3d(\n",
        "        state_dict,\n",
        "        name_pt + \".branch_1.0\",\n",
        "        sess,\n",
        "        os.path.join(name_tf, \"Branch_1/Conv3d_0a_1x1\"),\n",
        "    )\n",
        "    load_conv3d(\n",
        "        state_dict,\n",
        "        name_pt + \".branch_1.1\",\n",
        "        sess,\n",
        "        os.path.join(name_tf, \"Branch_1/Conv3d_0b_3x3\"),\n",
        "    )\n",
        "\n",
        "    # Branch 2\n",
        "    load_conv3d(\n",
        "        state_dict,\n",
        "        name_pt + \".branch_2.0\",\n",
        "        sess,\n",
        "        os.path.join(name_tf, \"Branch_2/Conv3d_0a_1x1\"),\n",
        "    )\n",
        "    if fix_typo:\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            name_pt + \".branch_2.1\",\n",
        "            sess,\n",
        "            os.path.join(name_tf, \"Branch_2/Conv3d_0a_3x3\"),\n",
        "        )\n",
        "    else:\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            name_pt + \".branch_2.1\",\n",
        "            sess,\n",
        "            os.path.join(name_tf, \"Branch_2/Conv3d_0b_3x3\"),\n",
        "        )\n",
        "\n",
        "    # Branch 3\n",
        "    load_conv3d(\n",
        "        state_dict,\n",
        "        name_pt + \".branch_3.1\",\n",
        "        sess,\n",
        "        os.path.join(name_tf, \"Branch_3/Conv3d_0b_1x1\"),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zyj4bfdmOLY5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pickle as pkl\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "\n",
        "\"\"\"\n",
        "Preforms a single inference forward pass given a video chunk (array of images)\n",
        "\n",
        "Arguments:\n",
        "    images: numpy array of images (video chunk), dimensionality TxHxWxC, first dimension is the time\n",
        "    network: a pytorch model\n",
        "    annotation_converter: an array of strings for mapping predicted IDs to class names (see load_model function, whichreads the annotation converter)\n",
        "    cuda_active (optional): default - True\n",
        "    reset_transform (torchvision.transform, optional): default - None. Resets the default transform with the specified reset_transform, if it is not None.\n",
        "\n",
        "Returns: top1_class, top1_class_conf, all_class_conf\n",
        "    top1_class (str): predicted top1 class\n",
        "    top1_class_conf (float): confidence of the predicted top1 class\n",
        "    all_class_conf (dict): confidences for all classes as a dict (key is the class name, value is the confidence)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_inference_on_video_chunk(\n",
        "    images,\n",
        "    network,\n",
        "    annotation_converter,\n",
        "    cuda_active=True,\n",
        "    reset_transform=None,\n",
        "):\n",
        "    if reset_transform:\n",
        "        transform = reset_transform\n",
        "    else:  # default transform\n",
        "        transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                Rescale(size=(252, 256)),\n",
        "                RandomSelect(n=32),\n",
        "                CenterCrop(height=224, width=224),\n",
        "                normalizeColorInputZeroCenterUnitRange(),\n",
        "                ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    images_transformed = transform(np.asarray(images))\n",
        "    images_transformed = images_transformed.unsqueeze(0)\n",
        "\n",
        "    if cuda_active:\n",
        "        images_transformed = Variable(images_transformed.cuda())\n",
        "        outputs = network(images_transformed).cuda()\n",
        "        outputs = np.squeeze(outputs.data.cpu().numpy())\n",
        "    else:\n",
        "        images_transformed = Variable(images_transformed)\n",
        "        outputs = network(images_transformed)\n",
        "        outputs = np.squeeze(outputs.data.numpy())\n",
        "\n",
        "    out_class_id = np.argmax(outputs)\n",
        "    top1_class_conf = np.max(outputs)\n",
        "\n",
        "    top1_class = annotation_converter[out_class_id]\n",
        "    all_class_conf = dict(zip(annotation_converter, outputs))\n",
        "\n",
        "    return top1_class, top1_class_conf, all_class_conf, outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Loads n_frames from video file at filepath as a numpy array, starting at start_frame and returns it as a numpy array.\n",
        "n_frames is 0 if all the frames should be taken\n",
        "Note, the video segment should not be too large (be careful with the default parameters, when n_frames = 0, meanining all frames are loaded into memory).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_video_segment(\n",
        "    filepath, start_frame=0, n_frames=0, visualize=False, waitKey=100\n",
        "):\n",
        "    cap = cv2.VideoCapture(filepath)\n",
        "\n",
        "    if start_frame > 0:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "    # Interate over frames in video\n",
        "    images = []\n",
        "\n",
        "    count = 0\n",
        "    if n_frames > 0:\n",
        "        video_length = n_frames\n",
        "    else:\n",
        "        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - start_frame - 1\n",
        "\n",
        "    while cap.isOpened():\n",
        "        # Extract the frame\n",
        "        ret, image = cap.read()\n",
        "        images.append(image)\n",
        "\n",
        "        if visualize:\n",
        "            cv2.imshow(\"Video Frame \", image)\n",
        "            cv2.waitKey(waitKey)\n",
        "\n",
        "        count = count + 1\n",
        "        # If there are no more frames left\n",
        "        if count > video_length - 1:\n",
        "            cap.release()\n",
        "            break\n",
        "        # print(count)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Iterate through a video file, compute activity predictions and visualize them in the video\n",
        "\n",
        "Arguments:\n",
        "    filepath: path for the video file\n",
        "    network: a pytorch model\n",
        "    annotation_converter: an array of strings for mapping predicted IDs to class names (see load_model function, whichreads the annotation converter)\n",
        "    start_frame (default 0): first frame of the video segment, for which the predictions should be computed\n",
        "    n_frames (default 0, depicting the complete video): number of frames, for which the predictions should be computed (0 if the complete video should be used)\n",
        "    visualize : default - False. Whether the video with the prediciotn should be visualized using opencv\n",
        "    waitKey : default - 100. Parameter for visualization. 100 means, visualization with 100 ms pause between the frames\n",
        "    buffer_size : default - 32.  Size of the stored frame buffer. The prediction is done on the chunk of this size. This means, if the size is 32, the network prediction is computed from the last 32 frames.\n",
        "    cuda_active : default - True\n",
        "    frequency : default - 1. Number of frames, after which a new prediction is computed. 1 means, a prediction is done after every frame.\n",
        "    video_path_out: default - None. If None, a the original video is re-written together with the prediction to the video_path_out\n",
        "    out_fps: default - 15, output fps\n",
        "    vidwriter: None -  optionally, one can already provide a vidwriter for the output video. If None, and video_path_out is not None, a new one is created.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# n_frames is 0 if all the frames should be taken\n",
        "def interate_video_and_predict(\n",
        "    filepath,\n",
        "    network,\n",
        "    annotation_converter,\n",
        "    start_frame=0,\n",
        "    n_frames=0,\n",
        "    visualize=False,\n",
        "    waitKey=100,\n",
        "    buffer_size=32,\n",
        "    cuda_active=True,\n",
        "    frequency=1,\n",
        "    video_path_out=None,\n",
        "    out_fps=15,\n",
        "    vidwriter=None,\n",
        "):\n",
        "    cap = cv2.VideoCapture(filepath)\n",
        "\n",
        "    if start_frame > 0:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "    # Interate over frames in video\n",
        "    images = []\n",
        "\n",
        "    count = 0\n",
        "    if n_frames > 0:\n",
        "        video_length = n_frames\n",
        "    else:\n",
        "        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - start_frame - 1\n",
        "    # if video_path_out:\n",
        "    #     os.makedirs(os.path.dirname(video_path_out), exist_ok=True)\n",
        "\n",
        "    all_tensors = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        # Extract the frame\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        images.append(image)\n",
        "        if len(images) > buffer_size:\n",
        "            images = images[len(images) - buffer_size : len(images)]\n",
        "\n",
        "        if count % frequency == 0:\n",
        "            top1_class, top1_class_conf, all_class_conf, outputs = (\n",
        "                run_inference_on_video_chunk(\n",
        "                    images,\n",
        "                    network,\n",
        "                    annotation_converter,\n",
        "                    cuda_active=cuda_active,\n",
        "                )\n",
        "            )\n",
        "            all_tensors.append(outputs)\n",
        "            # top1_class_conf_percent = round(100 * top1_class_conf, 2)\n",
        "\n",
        "            # fontColor = (\n",
        "            #     60 + 100 - top1_class_conf_percent,\n",
        "            #     1.8 * top1_class_conf_percent,\n",
        "            #     0,\n",
        "            # )\n",
        "\n",
        "            # image_without_text = image\n",
        "            # image = add_text_to_image(\n",
        "            #     image,\n",
        "            #     \"Frame nr: {}\".format(count),\n",
        "            #     bottomLeftCornerOfText=(10, 20),\n",
        "            #     fontScale=0.85,\n",
        "            # )\n",
        "            # image = add_text_to_image(\n",
        "            #     image,\n",
        "            #     \"{}%\".format(top1_class_conf_percent),\n",
        "            #     bottomLeftCornerOfText=(10, np.shape(image)[0] - 35),\n",
        "            #     fontColor=fontColor,\n",
        "            #     lineType=1,\n",
        "            #     fontScale=0.85,\n",
        "            #     font=cv2.FONT_HERSHEY_DUPLEX,\n",
        "            # )\n",
        "            # image = add_text_to_image(\n",
        "            #     image,\n",
        "            #     \"{}\".format(top1_class),\n",
        "            #     bottomLeftCornerOfText=(10, np.shape(image)[0] - 15),\n",
        "            #     fontColor=fontColor,\n",
        "            #     lineType=1,\n",
        "            #     fontScale=0.85,\n",
        "            #     font=cv2.FONT_HERSHEY_DUPLEX,\n",
        "            # )\n",
        "\n",
        "            # if visualize:\n",
        "            #     cv2.imshow(\"Video Frame \", image)\n",
        "            #     cv2.waitKey(waitKey)\n",
        "\n",
        "            # if video_path_out:\n",
        "\n",
        "            #     if vidwriter is None:\n",
        "            #         # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "            #         fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "            #         vidwriter = cv2.VideoWriter(\n",
        "            #             video_path_out,\n",
        "            #             fourcc,\n",
        "            #             out_fps,\n",
        "            #             (np.shape(image)[1], np.shape(image)[0]),\n",
        "            #         )\n",
        "\n",
        "            #     vidwriter.write(image)\n",
        "\n",
        "        count = count + 1\n",
        "        if count % 5000 == 0:\n",
        "          print(\"Current frame number: {}/{}\".format(count, n_frames))\n",
        "        # If there are no more frames left\n",
        "        if count > video_length - 1:\n",
        "\n",
        "            cap.release()\n",
        "            if vidwriter is not None:\n",
        "                vidwriter.release()\n",
        "\n",
        "            break\n",
        "    torch.save(all_tensors, f\"{video_path_out}.pt\")\n",
        "\n",
        "\n",
        "def load_model(trained_model_path, annotation_converter_path, cuda_active=True):\n",
        "    annotation_converter = pkl.load(open(annotation_converter_path, \"rb\"))\n",
        "    # Load the network\n",
        "    network = I3D(len(annotation_converter), modality=\"rgb\")\n",
        "\n",
        "    print(\"Loading trained model: %s\" % (trained_model_path))\n",
        "    if cuda_active:\n",
        "        network.load_state_dict(torch.load(trained_model_path))\n",
        "    else:\n",
        "        network.load_state_dict(\n",
        "            torch.load(trained_model_path, map_location=\"cpu\")\n",
        "        )\n",
        "    print(\n",
        "        \"Loading trained model done. Number of classes: {}\".format(\n",
        "            len(annotation_converter)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if cuda_active:\n",
        "        network = network.cuda()\n",
        "\n",
        "    torch.set_grad_enabled(False)\n",
        "    network.eval()\n",
        "\n",
        "    return network, annotation_converter\n",
        "\n",
        "\n",
        "def test_interate_video_and_predict(\n",
        "    filepath_video,\n",
        "    video_path_out,\n",
        "    start_frame=100,\n",
        "    n_frames=200,\n",
        "    cuda_active=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Iteratively process and predict over a video using a pre-trained model.\n",
        "    Improtant: if n_frames = 0 - the whole video is processed!\n",
        "\n",
        "    Parameters:\n",
        "    filepath_video (str): Path to the video file to be processed.\n",
        "    start_frame (int): The frame number from where to start the prediction.\n",
        "    n_frames (int): The number of frames to process and predict. Improtant: if 0 - the whole video is processed!\n",
        "\n",
        "    Returns:\n",
        "    None - Depending on the function's parameters, it may save a video with predictions or display the visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the model\n",
        "    trained_model_path = \"/content/best_model.pth\"\n",
        "    annotation_converter_path = \"/content/annotation_converter.pkl\"\n",
        "    network, annotation_converter = load_model(\n",
        "        trained_model_path, annotation_converter_path, cuda_active=cuda_active\n",
        "    )\n",
        "\n",
        "    # Predict for first 1000 frames\n",
        "    # Note: out_fps must be 15 for RGB videos!\n",
        "    interate_video_and_predict(\n",
        "        filepath_video,\n",
        "        network=network,\n",
        "        annotation_converter=annotation_converter,\n",
        "        start_frame=start_frame,\n",
        "        n_frames=n_frames,\n",
        "        visualize=False,\n",
        "        waitKey=100,\n",
        "        buffer_size=32,\n",
        "        cuda_active=cuda_active,\n",
        "        frequency=1,\n",
        "        video_path_out=video_path_out,\n",
        "        out_fps=15,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "url = \"https://driveandact.com/dataset/kinect_color.zip\"\n",
        "zip_file_path = \"/content/kinect_color.zip\"\n",
        "extraction_path = \"/content/kinect_color\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(zip_file_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(\"Extracted files:\")\n",
        "for root, dirs, files in os.walk(extraction_path):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ],
      "metadata": {
        "id": "xNaNGXzVfdK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "result_folder = 'result'\n",
        "os.makedirs(result_folder, exist_ok=True)\n",
        "\n",
        "for i in range(1, 16):\n",
        "    subfolder_name = f'vp{i}'\n",
        "    os.makedirs(os.path.join(result_folder, subfolder_name), exist_ok=True)\n",
        "\n",
        "print(f\"Created '{result_folder}' folder with subfolders vp1 to vp15.\")"
      ],
      "metadata": {
        "id": "lxZqrk8EupxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def inference_all(folder_path):\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".mp4\"):\n",
        "                folder_name = os.path.basename(root)\n",
        "                file_path = f\"/content/kinect_color/kinect_color/{folder_name}/{file}\"\n",
        "                output_path = (\n",
        "                    f\"/content/result/{folder_name}/{os.path.splitext(file)[0]}\"\n",
        "                )\n",
        "                if folder_name in ['vp4','vp5','vp8','vp12','vp13','vp14','vp15']:\n",
        "                  continue\n",
        "                print(f\"Processing: {file_path}\")\n",
        "                test_interate_video_and_predict(\n",
        "                    filepath_video=file_path,\n",
        "                    video_path_out=output_path,\n",
        "                    start_frame=0,\n",
        "                    n_frames=0,\n",
        "                    cuda_active=True,\n",
        "                )\n",
        "\n",
        "folder_name = \"/content/kinect_color/kinect_color\"\n",
        "inference_all(folder_name)"
      ],
      "metadata": {
        "id": "1TtNEc6ngdPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}