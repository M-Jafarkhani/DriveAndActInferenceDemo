{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W2ls4Yf62csX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import ReplicationPad3d\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "#from torch._six import container_abcs\n",
        "import collections.abc as container_abcs\n",
        "from itertools import islice\n",
        "import operator\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class ModuleList(torch.nn.Module):\n",
        "    r\"\"\"Holds submodules in a list.\n",
        "\n",
        "    ModuleList can be indexed like a regular Python list, but modules it\n",
        "    contains are properly registered, and will be visible by all Module methods.\n",
        "\n",
        "    Arguments:\n",
        "        modules (iterable, optional): an iterable of modules to add\n",
        "df\n",
        "    Example::\n",
        "\n",
        "        class MyModule(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(MyModule, self).__init__()\n",
        "                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
        "\n",
        "            def forward(self, x):\n",
        "                # ModuleList can act as an iterable, or be indexed using ints\n",
        "                for i, l in enumerate(self.linears):\n",
        "                    x = self.linears[i // 2](x) + l(x)\n",
        "                return x\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modules=None):\n",
        "        super(ModuleList, self).__init__()\n",
        "        if modules is not None:\n",
        "            self += modules\n",
        "\n",
        "    def _get_abs_string_index(self, idx):\n",
        "        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
        "        idx = operator.index(idx)\n",
        "        if not (-len(self) <= idx < len(self)):\n",
        "            raise IndexError('index {} is out of range'.format(idx))\n",
        "        if idx < 0:\n",
        "            idx += len(self)\n",
        "        return str(idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            return self.__class__(list(self._modules.values())[idx])\n",
        "        else:\n",
        "            return self._modules[self._get_abs_string_index(idx)]\n",
        "\n",
        "    def __setitem__(self, idx, module):\n",
        "        idx = self._get_abs_string_index(idx)\n",
        "        return setattr(self, str(idx), module)\n",
        "\n",
        "    def __delitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            for k in range(len(self._modules))[idx]:\n",
        "                delattr(self, str(k))\n",
        "        else:\n",
        "            delattr(self, self._get_abs_string_index(idx))\n",
        "        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n",
        "        str_indices = [str(i) for i in range(len(self._modules))]\n",
        "        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._modules)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self._modules.values())\n",
        "\n",
        "    def __iadd__(self, modules):\n",
        "        return self.extend(modules)\n",
        "\n",
        "    def __dir__(self):\n",
        "        keys = super(ModuleList, self).__dir__()\n",
        "        keys = [key for key in keys if not key.isdigit()]\n",
        "        return keys\n",
        "\n",
        "    def insert(self, index, module):\n",
        "        r\"\"\"Insert a given module before a given index in the list.\n",
        "\n",
        "        Arguments:\n",
        "            index (int): index to insert.\n",
        "            module (nn.Module): module to insert\n",
        "        \"\"\"\n",
        "        for i in range(len(self._modules), index, -1):\n",
        "            self._modules[str(i)] = self._modules[str(i - 1)]\n",
        "        self._modules[str(index)] = module\n",
        "\n",
        "    def append(self, module):\n",
        "        r\"\"\"Appends a given module to the end of the list.\n",
        "\n",
        "        Arguments:\n",
        "            module (nn.Module): module to append\n",
        "        \"\"\"\n",
        "        self.add_module(str(len(self)), module)\n",
        "        return self\n",
        "\n",
        "    def extend(self, modules):\n",
        "        r\"\"\"Appends modules from a Python iterable to the end of the list.\n",
        "\n",
        "        Arguments:\n",
        "            modules (iterable): iterable of modules to append\n",
        "        \"\"\"\n",
        "        if not isinstance(modules, container_abcs.Iterable):\n",
        "            raise TypeError(\"ModuleList.extend should be called with an \"\n",
        "                            \"iterable, but got \" + type(modules).__name__)\n",
        "        offset = len(self)\n",
        "        for i, module in enumerate(modules):\n",
        "            self.add_module(str(offset + i), module)\n",
        "        return self\n",
        "\n",
        "\n",
        "def get_padding_shape(filter_shape, stride):\n",
        "    def _pad_top_bottom(filter_dim, stride_val):\n",
        "        pad_along = max(filter_dim - stride_val, 0)\n",
        "        pad_top = pad_along // 2\n",
        "        pad_bottom = pad_along - pad_top\n",
        "        return pad_top, pad_bottom\n",
        "\n",
        "    padding_shape = []\n",
        "    for filter_dim, stride_val in zip(filter_shape, stride):\n",
        "        pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)\n",
        "        padding_shape.append(pad_top)\n",
        "        padding_shape.append(pad_bottom)\n",
        "    depth_top = padding_shape.pop(0)\n",
        "    depth_bottom = padding_shape.pop(0)\n",
        "    padding_shape.append(depth_top)\n",
        "    padding_shape.append(depth_bottom)\n",
        "\n",
        "    return tuple(padding_shape)\n",
        "\n",
        "\n",
        "def simplify_padding(padding_shapes):\n",
        "    all_same = True\n",
        "    padding_init = padding_shapes[0]\n",
        "    for pad in padding_shapes[1:]:\n",
        "        if pad != padding_init:\n",
        "            all_same = False\n",
        "    return all_same, padding_init\n",
        "\n",
        "\n",
        "class Unit3Dpy(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=(1, 1, 1),\n",
        "                 stride=(1, 1, 1),\n",
        "                 activation='relu',\n",
        "                 padding='SAME',\n",
        "                 use_bias=False,\n",
        "                 use_bn=True):\n",
        "        super(Unit3Dpy, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.use_bn = use_bn\n",
        "        if padding == 'SAME':\n",
        "            padding_shape = get_padding_shape(kernel_size, stride)\n",
        "            simplify_pad, pad_size = simplify_padding(padding_shape)\n",
        "            self.simplify_pad = simplify_pad\n",
        "        elif padding == 'VALID':\n",
        "            padding_shape = 0\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'padding should be in [VALID|SAME] but got {}'.format(padding))\n",
        "\n",
        "        if padding == 'SAME':\n",
        "            if not simplify_pad:\n",
        "                self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n",
        "                self.conv3d = torch.nn.Conv3d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size,\n",
        "                    stride=stride,\n",
        "                    bias=use_bias)\n",
        "            else:\n",
        "                self.conv3d = torch.nn.Conv3d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size,\n",
        "                    stride=stride,\n",
        "                    padding=pad_size,\n",
        "                    bias=use_bias)\n",
        "        elif padding == 'VALID':\n",
        "            self.conv3d = torch.nn.Conv3d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                padding=padding_shape,\n",
        "                stride=stride,\n",
        "                bias=use_bias)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'padding should be in [VALID|SAME] but got {}'.format(padding))\n",
        "\n",
        "        if self.use_bn:\n",
        "            self.batch3d = torch.nn.BatchNorm3d(out_channels)\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if self.padding == 'SAME' and self.simplify_pad is False:\n",
        "            inp = self.pad(inp)\n",
        "        out = self.conv3d(inp)\n",
        "        if self.use_bn:\n",
        "            out = self.batch3d(out)\n",
        "        if self.activation is not None:\n",
        "            out = torch.nn.functional.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MaxPool3dTFPadding(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, stride=None, padding='SAME'):\n",
        "        super(MaxPool3dTFPadding, self).__init__()\n",
        "        if padding == 'SAME':\n",
        "            padding_shape = get_padding_shape(kernel_size, stride)\n",
        "            self.padding_shape = padding_shape\n",
        "            self.pad = torch.nn.ConstantPad3d(padding_shape, 0)\n",
        "        self.pool = torch.nn.MaxPool3d(kernel_size, stride, ceil_mode=True)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = self.pad(inp)\n",
        "        out = self.pool(inp)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Mixed, self).__init__()\n",
        "        # Branch 0\n",
        "        self.branch_0 = Unit3Dpy(\n",
        "            in_channels, out_channels[0], kernel_size=(1, 1, 1))\n",
        "\n",
        "        # Branch 1\n",
        "        branch_1_conv1 = Unit3Dpy(\n",
        "            in_channels, out_channels[1], kernel_size=(1, 1, 1))\n",
        "        branch_1_conv2 = Unit3Dpy(\n",
        "            out_channels[1], out_channels[2], kernel_size=(3, 3, 3))\n",
        "        self.branch_1 = torch.nn.Sequential(branch_1_conv1, branch_1_conv2)\n",
        "\n",
        "        # Branch 2\n",
        "        branch_2_conv1 = Unit3Dpy(\n",
        "            in_channels, out_channels[3], kernel_size=(1, 1, 1))\n",
        "        branch_2_conv2 = Unit3Dpy(\n",
        "            out_channels[3], out_channels[4], kernel_size=(3, 3, 3))\n",
        "        self.branch_2 = torch.nn.Sequential(branch_2_conv1, branch_2_conv2)\n",
        "\n",
        "        # Branch3\n",
        "        branch_3_pool = MaxPool3dTFPadding(\n",
        "            kernel_size=(3, 3, 3), stride=(1, 1, 1), padding='SAME')\n",
        "        branch_3_conv2 = Unit3Dpy(\n",
        "            in_channels, out_channels[5], kernel_size=(1, 1, 1))\n",
        "        self.branch_3 = torch.nn.Sequential(branch_3_pool, branch_3_conv2)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out_0 = self.branch_0(inp)\n",
        "        out_1 = self.branch_1(inp)\n",
        "        out_2 = self.branch_2(inp)\n",
        "        out_3 = self.branch_3(inp)\n",
        "        out = torch.cat((out_0, out_1, out_2, out_3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class I3D(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 modality='rgb',\n",
        "                 dropout_prob=0,\n",
        "                 name='inception'):\n",
        "        super(I3D, self).__init__()\n",
        "\n",
        "        self.name = name\n",
        "        self.num_classes = num_classes\n",
        "        if modality == 'rgb':\n",
        "            in_channels = 3\n",
        "        elif modality == 'flow':\n",
        "            in_channels = 2\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                '{} not among known modalities [rgb|flow]'.format(modality))\n",
        "        self.modality = modality\n",
        "\n",
        "        conv3d_1a_7x7 = Unit3Dpy(\n",
        "            out_channels=64,\n",
        "            in_channels=in_channels,\n",
        "            kernel_size=(7, 7, 7),\n",
        "            stride=(2, 2, 2),\n",
        "            padding='SAME')\n",
        "        # 1st conv-pool\n",
        "        self.conv3d_1a_7x7 = conv3d_1a_7x7\n",
        "        self.maxPool3d_2a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')\n",
        "        # conv conv\n",
        "        conv3d_2b_1x1 = Unit3Dpy(\n",
        "            out_channels=64,\n",
        "            in_channels=64,\n",
        "            kernel_size=(1, 1, 1),\n",
        "            padding='SAME')\n",
        "        self.conv3d_2b_1x1 = conv3d_2b_1x1\n",
        "        conv3d_2c_3x3 = Unit3Dpy(\n",
        "            out_channels=192,\n",
        "            in_channels=64,\n",
        "            kernel_size=(3, 3, 3),\n",
        "            padding='SAME')\n",
        "        self.conv3d_2c_3x3 = conv3d_2c_3x3\n",
        "        self.maxPool3d_3a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')\n",
        "\n",
        "        # Mixed_3b\n",
        "        self.mixed_3b = Mixed(192, [64, 96, 128, 16, 32, 32])\n",
        "        self.mixed_3c = Mixed(256, [128, 128, 192, 32, 96, 64])\n",
        "\n",
        "        self.maxPool3d_4a_3x3 = MaxPool3dTFPadding(\n",
        "            kernel_size=(3, 3, 3), stride=(2, 2, 2), padding='SAME')\n",
        "\n",
        "        # Mixed 4\n",
        "        self.mixed_4b = Mixed(480, [192, 96, 208, 16, 48, 64])\n",
        "        self.mixed_4c = Mixed(512, [160, 112, 224, 24, 64, 64])\n",
        "        self.mixed_4d = Mixed(512, [128, 128, 256, 24, 64, 64])\n",
        "        self.mixed_4e = Mixed(512, [112, 144, 288, 32, 64, 64])\n",
        "        self.mixed_4f = Mixed(528, [256, 160, 320, 32, 128, 128])\n",
        "\n",
        "        self.maxPool3d_5a_2x2 = MaxPool3dTFPadding(\n",
        "            kernel_size=(2, 2, 2), stride=(2, 2, 2), padding='SAME')\n",
        "\n",
        "        # Mixed 5\n",
        "        self.mixed_5b = Mixed(832, [256, 160, 320, 32, 128, 128])\n",
        "        self.mixed_5c = Mixed(832, [384, 192, 384, 48, 128, 128])\n",
        "\n",
        "        self.avg_pool = torch.nn.AvgPool3d((2, 7, 7), (1, 1, 1))\n",
        "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "        self.conv3d_0c_1x1 = Unit3Dpy(\n",
        "            in_channels=1024,\n",
        "            out_channels=self.num_classes,\n",
        "            kernel_size=(1, 1, 1),\n",
        "            activation=None,\n",
        "            use_bias=True,\n",
        "            use_bn=False)\n",
        "        self.softmax = torch.nn.Softmax(1)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Preprocessing\n",
        "        #print(\"Input shape: {}\".format(inp.size()))\n",
        "        out = self.conv3d_1a_7x7(inp)\n",
        "\n",
        "        #print(\"Shape after out = self.conv3d_1a_7x7(inp): {}\".format(out.size()))\n",
        "        out = self.maxPool3d_2a_3x3(out)\n",
        "        out = self.conv3d_2b_1x1(out)\n",
        "        out = self.conv3d_2c_3x3(out)\n",
        "        out = self.maxPool3d_3a_3x3(out)\n",
        "        out = self.mixed_3b(out)\n",
        "        out = self.mixed_3c(out)\n",
        "        out = self.maxPool3d_4a_3x3(out)\n",
        "        #print(\"Shape after out = self.maxPool3d_4a_3x3(inp): {}\".format(out.size()))\n",
        "        out = self.mixed_4b(out)\n",
        "        out = self.mixed_4c(out)\n",
        "        out = self.mixed_4d(out)\n",
        "        out = self.mixed_4e(out)\n",
        "        out = self.mixed_4f(out)\n",
        "        out = self.maxPool3d_5a_2x2(out)\n",
        "        out = self.mixed_5b(out)\n",
        "        out = self.mixed_5c(out)\n",
        "        #print(\"Shape after out = self.mixed_5c(out): {}\".format(out.size()))\n",
        "        out = self.avg_pool(out)\n",
        "        #print(\"Shape after self.avg_pool(out: {}\".format(out.size()))\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv3d_0c_1x1(out)\n",
        "        #\n",
        "        #print(\"Shape after out = self.conv3d_0c_1x1(out)): {}\".format(out.size()))\n",
        "\n",
        "        out = out.squeeze(3)\n",
        "        out = out.squeeze(3)\n",
        "        #print (\"Shaper after out = out.squeeze(3): {}\".format(out.size()))\n",
        "        out = out.mean(2)\n",
        "        #print(\"Shaper after out = out.mean(2): {}\".format(out.size()))\n",
        "\n",
        "        out_logits = out\n",
        "        out = self.softmax(out_logits)\n",
        "        #return out, out_logits\n",
        "        return out\n",
        "\n",
        "    def load_tf_weights(self, sess):\n",
        "        state_dict = {}\n",
        "        if self.modality == 'rgb':\n",
        "            prefix = 'RGB/inception_i3d'\n",
        "        elif self.modality == 'flow':\n",
        "            prefix = 'Flow/inception_i3d'\n",
        "        load_conv3d(state_dict, 'conv3d_1a_7x7', sess,\n",
        "                    os.path.join(prefix, 'Conv3d_1a_7x7'))\n",
        "        load_conv3d(state_dict, 'conv3d_2b_1x1', sess,\n",
        "                    os.path.join(prefix, 'Conv3d_2b_1x1'))\n",
        "        load_conv3d(state_dict, 'conv3d_2c_3x3', sess,\n",
        "                    os.path.join(prefix, 'Conv3d_2c_3x3'))\n",
        "\n",
        "        load_mixed(state_dict, 'mixed_3b', sess,\n",
        "                   os.path.join(prefix, 'Mixed_3b'))\n",
        "        load_mixed(state_dict, 'mixed_3c', sess,\n",
        "                   os.path.join(prefix, 'Mixed_3c'))\n",
        "        load_mixed(state_dict, 'mixed_4b', sess,\n",
        "                   os.path.join(prefix, 'Mixed_4b'))\n",
        "        load_mixed(state_dict, 'mixed_4c', sess,\n",
        "                   os.path.join(prefix, 'Mixed_4c'))\n",
        "        load_mixed(state_dict, 'mixed_4d', sess,\n",
        "                   os.path.join(prefix, 'Mixed_4d'))\n",
        "        load_mixed(state_dict, 'mixed_4e', sess,\n",
        "                   os.path.join(prefix, 'Mixed_4e'))\n",
        "        # Here goest to 0.1 max error with tf\n",
        "        load_mixed(state_dict, 'mixed_4f', sess,\n",
        "                   os.path.join(prefix, 'Mixed_4f'))\n",
        "\n",
        "        load_mixed(\n",
        "            state_dict,\n",
        "            'mixed_5b',\n",
        "            sess,\n",
        "            os.path.join(prefix, 'Mixed_5b'),\n",
        "            fix_typo=True)\n",
        "        load_mixed(state_dict, 'mixed_5c', sess,\n",
        "                   os.path.join(prefix, 'Mixed_5c'))\n",
        "        load_conv3d(\n",
        "            state_dict,\n",
        "            'conv3d_0c_1x1',\n",
        "            sess,\n",
        "            os.path.join(prefix, 'Logits', 'Conv3d_0c_1x1'),\n",
        "            bias=True,\n",
        "            bn=False)\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def get_conv_params(sess, name, bias=False):\n",
        "    # Get conv weights\n",
        "    conv_weights_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, 'w:0'))\n",
        "    if bias:\n",
        "        conv_bias_tensor = sess.graph.get_tensor_by_name(\n",
        "            os.path.join(name, 'b:0'))\n",
        "        conv_bias = sess.run(conv_bias_tensor)\n",
        "    conv_weights = sess.run(conv_weights_tensor)\n",
        "    conv_shape = conv_weights.shape\n",
        "\n",
        "    kernel_shape = conv_shape[0:3]\n",
        "    in_channels = conv_shape[3]\n",
        "    out_channels = conv_shape[4]\n",
        "\n",
        "    conv_op = sess.graph.get_operation_by_name(\n",
        "        os.path.join(name, 'convolution'))\n",
        "    padding_name = conv_op.get_attr('padding')\n",
        "    padding = _get_padding(padding_name, kernel_shape)\n",
        "    all_strides = conv_op.get_attr('strides')\n",
        "    strides = all_strides[1:4]\n",
        "    conv_params = [\n",
        "        conv_weights, kernel_shape, in_channels, out_channels, strides, padding\n",
        "    ]\n",
        "    if bias:\n",
        "        conv_params.append(conv_bias)\n",
        "    return conv_params\n",
        "\n",
        "\n",
        "def get_bn_params(sess, name):\n",
        "    moving_mean_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, 'moving_mean:0'))\n",
        "    moving_var_tensor = sess.graph.get_tensor_by_name(\n",
        "        os.path.join(name, 'moving_variance:0'))\n",
        "    beta_tensor = sess.graph.get_tensor_by_name(os.path.join(name, 'beta:0'))\n",
        "    moving_mean = sess.run(moving_mean_tensor)\n",
        "    moving_var = sess.run(moving_var_tensor)\n",
        "    beta = sess.run(beta_tensor)\n",
        "    return moving_mean, moving_var, beta\n",
        "\n",
        "\n",
        "def _get_padding(padding_name, conv_shape):\n",
        "    padding_name = padding_name.decode(\"utf-8\")\n",
        "    if padding_name == \"VALID\":\n",
        "        return [0, 0]\n",
        "    elif padding_name == \"SAME\":\n",
        "        #return [math.ceil(int(conv_shape[0])/2), math.ceil(int(conv_shape[1])/2)]\n",
        "        return [\n",
        "            math.floor(int(conv_shape[0]) / 2),\n",
        "            math.floor(int(conv_shape[1]) / 2),\n",
        "            math.floor(int(conv_shape[2]) / 2)\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError('Invalid padding name ' + padding_name)\n",
        "\n",
        "\n",
        "def load_conv3d(state_dict, name_pt, sess, name_tf, bias=False, bn=True):\n",
        "    # Transfer convolution params\n",
        "    conv_name_tf = os.path.join(name_tf, 'conv_3d')\n",
        "    conv_params = get_conv_params(sess, conv_name_tf, bias=bias)\n",
        "    if bias:\n",
        "        conv_weights, kernel_shape, in_channels, out_channels, strides, padding, conv_bias = conv_params\n",
        "    else:\n",
        "        conv_weights, kernel_shape, in_channels, out_channels, strides, padding = conv_params\n",
        "\n",
        "    conv_weights_rs = np.transpose(\n",
        "        conv_weights, (4, 3, 0, 1,\n",
        "                       2))  # to pt format (out_c, in_c, depth, height, width)\n",
        "    state_dict[name_pt + '.conv3d.weight'] = torch.from_numpy(conv_weights_rs)\n",
        "    if bias:\n",
        "        state_dict[name_pt + '.conv3d.bias'] = torch.from_numpy(conv_bias)\n",
        "\n",
        "    # Transfer batch norm params\n",
        "    if bn:\n",
        "        conv_tf_name = os.path.join(name_tf, 'batch_norm')\n",
        "        moving_mean, moving_var, beta = get_bn_params(sess, conv_tf_name)\n",
        "\n",
        "        out_planes = conv_weights_rs.shape[0]\n",
        "        state_dict[name_pt + '.batch3d.weight'] = torch.ones(out_planes)\n",
        "        state_dict[name_pt + '.batch3d.bias'] = torch.from_numpy(beta)\n",
        "        state_dict[name_pt\n",
        "                   + '.batch3d.running_mean'] = torch.from_numpy(moving_mean)\n",
        "        state_dict[name_pt\n",
        "                   + '.batch3d.running_var'] = torch.from_numpy(moving_var)\n",
        "\n",
        "\n",
        "def load_mixed(state_dict, name_pt, sess, name_tf, fix_typo=False):\n",
        "    # Branch 0\n",
        "    load_conv3d(state_dict, name_pt + '.branch_0', sess,\n",
        "                os.path.join(name_tf, 'Branch_0/Conv3d_0a_1x1'))\n",
        "\n",
        "    # Branch .1\n",
        "    load_conv3d(state_dict, name_pt + '.branch_1.0', sess,\n",
        "                os.path.join(name_tf, 'Branch_1/Conv3d_0a_1x1'))\n",
        "    load_conv3d(state_dict, name_pt + '.branch_1.1', sess,\n",
        "                os.path.join(name_tf, 'Branch_1/Conv3d_0b_3x3'))\n",
        "\n",
        "    # Branch 2\n",
        "    load_conv3d(state_dict, name_pt + '.branch_2.0', sess,\n",
        "                os.path.join(name_tf, 'Branch_2/Conv3d_0a_1x1'))\n",
        "    if fix_typo:\n",
        "        load_conv3d(state_dict, name_pt + '.branch_2.1', sess,\n",
        "                    os.path.join(name_tf, 'Branch_2/Conv3d_0a_3x3'))\n",
        "    else:\n",
        "        load_conv3d(state_dict, name_pt + '.branch_2.1', sess,\n",
        "                    os.path.join(name_tf, 'Branch_2/Conv3d_0b_3x3'))\n",
        "\n",
        "    # Branch 3\n",
        "    load_conv3d(state_dict, name_pt + '.branch_3.1', sess,\n",
        "    os.path.join(name_tf, 'Branch_3/Conv3d_0b_1x1'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R8F4slBJ36nf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from random import randint, seed\n",
        "import math\n",
        "import os\n",
        "import PIL\n",
        "import numbers\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "def add_text_to_image(img, text, font = cv2.FONT_ITALIC, bottomLeftCornerOfText = (10,20), fontScale = 0.4,fontColor = (200,200,200),lineType = 1):\n",
        "\n",
        "    color = np.random.randint(0, 255, size=(3, ))\n",
        "    color = ( int (color [ 0 ]), int (color [ 1 ]), int (color [ 2 ]))\n",
        "    img_with_text = cv2.putText(img, text,\n",
        "                bottomLeftCornerOfText,\n",
        "                font,\n",
        "                fontScale,\n",
        "                color,\n",
        "                lineType)\n",
        "    return img_with_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extractFilesFromDirWhichMatchList(folder, string_match_filenames, string_not_match_filenames=[], full_path=False):\n",
        "    result_files = []\n",
        "    print (folder)\n",
        "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
        "        for filename in filenames:\n",
        "            ok = True\n",
        "\n",
        "            if full_path:\n",
        "\n",
        "                check = os.path.join(dirpath, filename)\n",
        "                # Only include if matches\n",
        "                for s in string_match_filenames:\n",
        "                    if (check.find(s) == -1):  # if not found\n",
        "                        ok = False\n",
        "\n",
        "                # Exclude if matches\n",
        "                for s in string_not_match_filenames:\n",
        "                    if (check.find(s) != -1):  # if found\n",
        "                        ok = False\n",
        "\n",
        "            else:\n",
        "                # Only include if matches\n",
        "                for s in string_match_filenames:\n",
        "                    if (filename.find(s) == -1):  # if not found\n",
        "                        ok = False\n",
        "\n",
        "                # Exclude if matches\n",
        "                for s in string_not_match_filenames:\n",
        "                    if (filename.find(s) != -1):  # if found\n",
        "                        ok = False\n",
        "\n",
        "            if ok:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                result_files.append(file_path)\n",
        "\n",
        "    return result_files\n",
        "\n",
        "\n",
        "def resize_clip(clip, size, interpolation='bilinear'):\n",
        "    if isinstance(clip[0], np.ndarray):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_h, im_w, im_c = clip[0].shape\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
        "                                                   and im_h == size):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "        if interpolation == 'bilinear':\n",
        "            np_inter = cv2.INTER_LINEAR\n",
        "        else:\n",
        "            np_inter = cv2.INTER_NEAREST\n",
        "        scaled = [\n",
        "            cv2.resize(img, size, interpolation=np_inter) for img in clip\n",
        "        ]\n",
        "    elif isinstance(clip[0], PIL.Image.Image):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            im_w, im_h = clip[0].size\n",
        "            # Min spatial dim already matches minimal size\n",
        "            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n",
        "                                                   and im_h == size):\n",
        "                return clip\n",
        "            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n",
        "            size = (new_w, new_h)\n",
        "        else:\n",
        "            size = size[1], size[0]\n",
        "        if interpolation == 'bilinear':\n",
        "            pil_inter = PIL.Image.BILINEAR\n",
        "        else:\n",
        "            pil_inter = PIL.Image.NEAREST\n",
        "        scaled = [img.resize(size, pil_inter) for img in clip]\n",
        "    else:\n",
        "        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n",
        "                        'but got list of {0}'.format(type(clip[0])))\n",
        "    return np.asarray(scaled)\n",
        "\n",
        "\n",
        "def get_resize_sizes(im_h, im_w, size):\n",
        "    if im_w < im_h:\n",
        "        ow = size\n",
        "        oh = int(size * im_h / im_w)\n",
        "    else:\n",
        "        oh = size\n",
        "        ow = int(size * im_w / im_h)\n",
        "    return oh, ow\n",
        "\n",
        "def normalize_color_input_zero_center_unit_range(frames, max_val = 255.0):\n",
        "\n",
        "    frames = (frames / max_val) * 2 - 1\n",
        "    return(frames)\n",
        "\n",
        "\n",
        "class normalizeColorInputZeroCenterUnitRange(object):\n",
        "    def __init__(self, max_val = 255.0):\n",
        "\n",
        "        self.max_val = max_val\n",
        "\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        result = normalize_color_input_zero_center_unit_range(input_tensor, max_val = self.max_val)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "def random_select(frames, n, seed = None):\n",
        "    \"\"\"\n",
        "    Takes multiple frames as ndarray with shape\n",
        "    (frame id, height, width, channels) and selects\n",
        "    randomly n-frames. If n is greater than the number\n",
        "    of overall frames, placeholder frames (zeros) will\n",
        "    be added.\n",
        "\n",
        "    frames: numpy\n",
        "        all frames (e.g. video) with shape\n",
        "        (frame id, height, width, channels)\n",
        "    n: int\n",
        "        number of desired randomly picked frames\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy: frames\n",
        "        randomly picked frames with shape\n",
        "        (frame id, height, width, channels)\n",
        "    \"\"\"\n",
        "    #print(\"Frames shape:{}\".format(np.shape(frames)))\n",
        "    if seed is not None:\n",
        "        seed(seed)\n",
        "\n",
        "    number_of_frames = np.shape(frames)[0]\n",
        "    if number_of_frames < n:\n",
        "        # Add all frames\n",
        "        selected_frames = []\n",
        "        for i in range(number_of_frames):\n",
        "            frame = frames[i, :, :, :]\n",
        "            selected_frames.append(frame)\n",
        "\n",
        "        # Fill up with 'placeholder' images\n",
        "        frame = np.zeros(frames[0, :, :, :].shape)\n",
        "        for i in range(n - number_of_frames):\n",
        "            selected_frames.append(frame)\n",
        "\n",
        "        return np.array(selected_frames)\n",
        "\n",
        "    # Selected random frame ids\n",
        "    frame_ids = set([])\n",
        "    while len(frame_ids) < n:\n",
        "        frame_ids.add(randint(0, number_of_frames - 1))\n",
        "\n",
        "    # Sort the frame ids\n",
        "    frame_ids = sorted(frame_ids)\n",
        "\n",
        "    # Select frames\n",
        "    selected_frames = []\n",
        "    for id in frame_ids:\n",
        "        #print (np.shape(frames))\n",
        "\n",
        "        frame = frames[id, :, :, :]\n",
        "        selected_frames.append(frame)\n",
        "\n",
        "    return np.array(selected_frames)\n",
        "\n",
        "\n",
        "def center_crop(frames, height, width, pad_zeros_if_too_small = True):\n",
        "    \"\"\"\n",
        "    Takes multiple frames as ndarray with shape\n",
        "    (frame id, height, width, channels) and crops all\n",
        "    frames centered to desired width and height.\n",
        "\n",
        "    frames: numpy\n",
        "        all frames (e.g. video) with shape\n",
        "        (frame id, height, width, channels)\n",
        "    height: int\n",
        "        height of the resulting crop\n",
        "    width: int\n",
        "        width of the resulting crop\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy: frames\n",
        "        centered cropped frames with shape\n",
        "        (frame id, height, width, channels)\n",
        "    \"\"\"\n",
        "\n",
        "    frame_height = np.shape(frames)[1]\n",
        "    frame_width = np.shape(frames)[2]\n",
        "\n",
        "    t = np.shape(frames)[0]\n",
        "    channels = np.shape(frames)[3]\n",
        "\n",
        "    if pad_zeros_if_too_small and (height > frame_height or width > frame_width):\n",
        "        # desired width\n",
        "        frames_new = np.zeros((t, max(frame_height, height), max(frame_width, width), channels))\n",
        "        # fill with the old data\n",
        "        frames_new[0:t, 0:frame_height, 0:frame_width, 0:channels] = frames\n",
        "        frames = frames_new\n",
        "        frame_height = np.shape(frames)[1]\n",
        "        frame_width = np.shape(frames)[2]\n",
        "\n",
        "\n",
        "    origin_x = (frame_width - width) / 2\n",
        "    origin_y = (frame_height - height) / 2\n",
        "\n",
        "    # Floor origin (miss matching input sizes)\n",
        "    # E.g. input width of 171 and crop width 112\n",
        "    # would result in a float.\n",
        "    origin_x = math.floor(origin_x)\n",
        "    origin_y = math.floor(origin_y)\n",
        "\n",
        "    return frames[:,\n",
        "                  origin_y: origin_y + height,\n",
        "                  origin_x: origin_x + width,\n",
        "                  :]\n",
        "\n",
        "\n",
        "\n",
        "class Rescale(object):\n",
        "    def __init__(self, size, interpolation='bilinear'):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, clip):\n",
        "\n",
        "        resized = resize_clip(\n",
        "            clip, self.size, interpolation=self.interpolation)\n",
        "        return resized\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, height, width):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "        result = center_crop(input_tensor, self.height, self.width)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RandomSelect(object):\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "        result = random_select(input_tensor, self.n)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, input_tensor):\n",
        "\n",
        "\n",
        "        # Swap color channels axis because\n",
        "        # numpy frames: Frames ID x Height x Width x Channels\n",
        "        # torch frames: Channels x Frame ID x Height x Width\n",
        "\n",
        "        result = input_tensor.transpose(3, 0, 1, 2)\n",
        "        result = np.float32(result)\n",
        "\n",
        "        return torch.from_numpy(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tejE3MAc3yL7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pickle as pkl\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "\"\"\"\n",
        "Preforms a single inference forward pass given a video chunk (array of images)\n",
        "\n",
        "Arguments:\n",
        "    images: numpy array of images (video chunk), dimensionality TxHxWxC, first dimension is the time\n",
        "    network: a pytorch model\n",
        "    annotation_converter: an array of strings for mapping predicted IDs to class names (see load_model function, which reads the annotation converter)\n",
        "    cuda_active (optional): default - True ( the model is moved to the GPU.)\n",
        "    reset_transform (torchvision.transform, optional): default - None. Resets the default transform with the specified reset_transform, if it is not None.\n",
        "\n",
        "Returns: top1_class, top1_class_conf, all_class_conf\n",
        "    top1_class (str): predicted top1 class\n",
        "    top1_class_conf (float): confidence of the predicted top1 class\n",
        "    all_class_conf (dict): confidences for all classes as a dict (key is the class name, value is the confidence)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def run_inference_on_video_chunk(images, network, annotation_converter, cuda_active=True, reset_transform=None):\n",
        "    if (reset_transform):\n",
        "        transform = reset_transform\n",
        "    else:  # default transform\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            Rescale(size=(252, 256)), #Resize frames\n",
        "            RandomSelect(n=32), #Randomly select n=32 frames if more frames provided\n",
        "            CenterCrop(height=224, width=224), #Crop frames to (224, 224)\n",
        "            normalizeColorInputZeroCenterUnitRange(), #Scale pixel values to the range [-1, 1]\n",
        "            ToTensor() #Convert the NumPy array to a PyTorch tensor\n",
        "        ])\n",
        "\n",
        "    images_transformed = transform(np.asarray(images)) #Converts the input images to a NumPy array and applies the preprocessing pipeline\n",
        "    images_transformed = images_transformed.unsqueeze(0) #Shape after unsqueeze(0): (1, C, T, H, W)\n",
        "\n",
        "    if cuda_active:\n",
        "        images_transformed = Variable(images_transformed.cuda())\n",
        "        outputs = network(images_transformed).cuda()  #pass the transformed input to the model\n",
        "        outputs = np.squeeze(outputs.data.cpu().numpy())\n",
        "    else:\n",
        "        images_transformed = Variable(images_transformed)\n",
        "        outputs = network(images_transformed)\n",
        "        outputs = np.squeeze(outputs.data.numpy()) #Converts predictions to a NumPy array and removes extra dimensions.\n",
        "\n",
        "    out_class_id = np.argmax(outputs)   # Index of the class with the highest score\n",
        "    top1_class_conf = np.max(outputs)   # Confidence of the highest score\n",
        "\n",
        "    top1_class = annotation_converter[out_class_id] #Convert the out_class_id to a human-readable activity using the annotation_converter.\n",
        "    all_class_conf = dict(zip(annotation_converter, outputs)) #dictionary mapping all class indices to their confidence scores\n",
        "\n",
        "    return top1_class, top1_class_conf, all_class_conf\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Loads n_frames from video file at filepath as a numpy array, starting at start_frame and returns it as a numpy array.\n",
        "n_frames is 0 if all the frames should be taken\n",
        "Note, the video segment should not be too large (be careful with the default parameters, when n_frames = 0, meanining all frames are loaded into memory).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_video_segment(filepath, start_frame=0, n_frames=0, visualize=False, waitKey=100):\n",
        "    cap = cv2.VideoCapture(filepath)\n",
        "\n",
        "    if (start_frame > 0):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame);\n",
        "\n",
        "    # Interate over frames in video\n",
        "    images = []\n",
        "\n",
        "    count = 0\n",
        "    if (n_frames > 0):\n",
        "        video_length = n_frames\n",
        "    else:\n",
        "        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - start_frame - 1\n",
        "\n",
        "    while cap.isOpened():\n",
        "        # Extract the frame\n",
        "        ret, image = cap.read()\n",
        "        images.append(image)\n",
        "\n",
        "        if (visualize):\n",
        "            cv2.imshow('Video Frame ', image)\n",
        "            cv2.waitKey(waitKey)\n",
        "\n",
        "        count = count + 1\n",
        "        # If there are no more frames left\n",
        "        if (count > video_length - 1):\n",
        "            cap.release()\n",
        "            break\n",
        "        # print(count)\n",
        "\n",
        "    return (images)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Iterate through a video file frame by frame, compute activity predictions and visualize them in the video\n",
        "Maintains buffer frames, uses pretrained I3D model to predict activities for the buffered frames.\n",
        "Anotates the frames.\n",
        "Arguments:\n",
        "    filepath: path for the video file\n",
        "    network: a pytorch model\n",
        "    annotation_converter: an array of strings for mapping predicted IDs to class names (see load_model function, whichreads the annotation converter)\n",
        "    start_frame (default 0): first frame of the video segment, for which the predictions should be computed\n",
        "    n_frames (default 0, depicting the complete video): number of frames, for which the predictions should be computed (0 if the complete video should be used)\n",
        "    visualize : default - False. Whether the video with the prediciotn should be visualized using opencv\n",
        "    waitKey : default - 100. Parameter for visualization. 100 means, visualization with 100 ms pause between the frames\n",
        "    buffer_size : default - 32.  Size of the stored frame buffer. The prediction is done on the chunk of this size. This means, if the size is 32, the network prediction is computed from the last 32 frames.\n",
        "    cuda_active : default - True\n",
        "    frequency : default - 1. Number of frames, after which a new prediction is computed. 1 means, a prediction is done after every frame.\n",
        "    video_path_out: default - None. If None, a the original video is re-written together with the prediction to the video_path_out\n",
        "    out_fps: default - 15, output fps\n",
        "    vidwriter: None -  optionally, one can already provide a vidwriter for the output video. If None, and video_path_out is not None, a new one is created.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# n_frames is 0 if all the frames should be taken\n",
        "def interate_video_and_predict(filepath, network, annotation_converter, start_frame=0, n_frames=0, visualize=False,\n",
        "                               waitKey=100,\n",
        "                               buffer_size=32, cuda_active=True, frequency=1, video_path_out=None, out_fps=15,\n",
        "                               vidwriter=None):\n",
        "    cap = cv2.VideoCapture(filepath)  #Opens the video file using OpenCV\n",
        "\n",
        "    if (start_frame > 0):  #If start_frame is specified, skips frames until the start_frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame);\n",
        "\n",
        "    # Interate over frames in video\n",
        "    images = []   #A buffer to store frames\n",
        "\n",
        "    count = 0\n",
        "    if (n_frames > 0):\n",
        "        video_length = n_frames         #If n_frames is provided, processes exactly n_frames frames\n",
        "    else:\n",
        "        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - start_frame - 1  #processes all frames from start_frame to the end of the video\n",
        "    if (video_path_out):\n",
        "        os.makedirs(os.path.dirname(video_path_out), exist_ok=True)\n",
        "    while cap.isOpened(): # Extract the frame\n",
        "        ret, image = cap.read()\n",
        "\n",
        "        images.append(image)  #Adds the current frame to the images buffer\n",
        "        if len(images) > buffer_size:\n",
        "            images = images[len(images) - buffer_size:len(images)]  #Trims the buffer to ensure it contains at most buffer_size frames.\n",
        "\n",
        "        if count % frequency == 0: # (frequency = 1, predictions are made for every frame)\n",
        "\n",
        "            top1_class, top1_class_conf, all_class_conf = run_inference_on_video_chunk(images, network,\n",
        "                                                                                       annotation_converter,                                                                           cuda_active=cuda_active)\n",
        "\n",
        "            top1_class_conf_percent = round(100 * top1_class_conf, 2)\n",
        "\n",
        "            fontColor = (60 + 100 - top1_class_conf_percent, 1.8 * top1_class_conf_percent, 0)\n",
        "\n",
        "            #Annotate the Frame\n",
        "            image_without_text = image\n",
        "            image = add_text_to_image(image, \"Frame nr: {}\".format(count),\n",
        "                                                          bottomLeftCornerOfText=(10, 20), fontScale=0.85)\n",
        "            image = add_text_to_image(image, \"{}%\".format(top1_class_conf_percent),\n",
        "                                                          bottomLeftCornerOfText=(10, np.shape(image)[0] - 35),\n",
        "                                                          fontColor=fontColor, lineType=1, fontScale=0.85,\n",
        "                                                          font=cv2.FONT_HERSHEY_DUPLEX)\n",
        "            image = add_text_to_image(image, \"{}\".format(top1_class),\n",
        "                                                          bottomLeftCornerOfText=(10, np.shape(image)[0] - 15),\n",
        "                                                          fontColor=fontColor,\n",
        "                                                          lineType=1, fontScale=0.85, font=cv2.FONT_HERSHEY_DUPLEX)\n",
        "            #Display the Frame (Optional)\n",
        "            if (visualize):\n",
        "                cv2.imshow('Video Frame ', image)\n",
        "                cv2.waitKey(waitKey)\n",
        "            #Save the Frame to Output Video\n",
        "            if (video_path_out):\n",
        "\n",
        "                if vidwriter is None:\n",
        "                    # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "                    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "                    vidwriter = cv2.VideoWriter(video_path_out, fourcc, out_fps,\n",
        "                                                (np.shape(image)[1], np.shape(image)[0]))\n",
        "\n",
        "                vidwriter.write(image)\n",
        "\n",
        "        count = count + 1\n",
        "        print(\"Current frame number: {}/{}\".format(count, n_frames))\n",
        "        # If there are no more frames left release the resources\n",
        "        # Stops processing after the specified number of frames (video_length)\n",
        "        if (count > video_length - 1):\n",
        "\n",
        "            cap.release()\n",
        "            if vidwriter is not None:\n",
        "                vidwriter.release()\n",
        "\n",
        "            break\n",
        "\n",
        "def load_model(trained_model_path, annotation_converter_path, cuda_active=True):\n",
        "    annotation_converter = pkl.load(open(annotation_converter_path, 'rb'))\n",
        "    # Load the network\n",
        "    network = I3D(len(annotation_converter), modality='rgb')\n",
        "\n",
        "    print(\"Loading trained model: %s\" % (trained_model_path))\n",
        "    if (cuda_active):\n",
        "        network.load_state_dict(torch.load(trained_model_path))\n",
        "    else:\n",
        "        network.load_state_dict(torch.load(trained_model_path, map_location='cpu'))\n",
        "    print(\"Loading trained model done. Number of classes: {}\".format(len(annotation_converter)))\n",
        "\n",
        "    if cuda_active:\n",
        "        network = network.cuda()\n",
        "\n",
        "    torch.set_grad_enabled(False)\n",
        "    network.eval()\n",
        "\n",
        "    return network, annotation_converter\n",
        "\n",
        "\n",
        "def test_run_inference_on_video_chunk():\n",
        "    cuda_active = True\n",
        "\n",
        "    # Load the model\n",
        "    trained_model_path = \"./demo_models/MidLevel/AllActions/view_ids_1/supervised_models/I3D/n_input_frames_32balance_by_sampling_True_affine_True_pretrained_True/2019-03-27-13-10-39/best_model.pth\"\n",
        "    annotation_converter_path = \"./demo_models/MidLevel/AllActions/view_ids_1/supervised_models/I3D/n_input_frames_32balance_by_sampling_True_affine_True_pretrained_True/2019-03-27-13-10-39/annotation_converter.pkl\"\n",
        "    network, annotation_converter = load_model(trained_model_path, annotation_converter_path, cuda_active=cuda_active)\n",
        "\n",
        "    # Load the video chunk (numpy array)\n",
        "    filepath_video = \"/cvhci/data/activity/Pakos/final_dataset/pakos_videos/vp1/run1b_2018-05-29-14-02-47.ids_1.mp4\"\n",
        "    start_frame = 30000\n",
        "    n_frames = 32\n",
        "    video_chunk = load_video_segment(filepath_video, start_frame=start_frame, n_frames=n_frames)\n",
        "\n",
        "    # Make a prediction\n",
        "    print(\"Computing prediction for video {}, video chunk from frame {} to frame {}\".format(filepath_video, start_frame,\n",
        "                                                                                            start_frame + n_frames))\n",
        "    top1_class, top1_class_conf, all_class_conf = run_inference_on_video_chunk(video_chunk, network,\n",
        "                                                                               annotation_converter,\n",
        "                                                                               cuda_active=cuda_active)\n",
        "\n",
        "    # Print all predictions sorted by confidence\n",
        "    ranks = np.argsort(list(all_class_conf.values()))[::-1]  # extract confidence scores and sort indices in descending order of confidence\n",
        "    classes_sorted = np.asarray(list(all_class_conf.keys()))[ranks] #Classes sorted by confidence\n",
        "    conf_sorted = np.asarray(list(all_class_conf.values()))[ranks] #Corresponding confidence scores\n",
        "\n",
        "    for i in range(len(conf_sorted)):\n",
        "        print(\"{}) {} - {}\".format(i + 1, classes_sorted[i], conf_sorted[i]))\n",
        "\n",
        "\n",
        "def test_interate_video_and_predict(filepath_video=\"./test_data/run1b_2018-05-29-14-02-47.kinect_color.mp4\",\n",
        "                                    video_path_out = \"./test_data/output.mp4\",\n",
        "                                    start_frame=100,\n",
        "                                    n_frames=200,\n",
        "                                    cuda_active = True):\n",
        "    \"\"\"\n",
        "    Iteratively process and predict over a video using a pre-trained model.\n",
        "    Improtant: if n_frames = 0 - the whole video is processed!\n",
        "\n",
        "    Parameters:\n",
        "    filepath_video (str): Path to the video file to be processed.\n",
        "    start_frame (int): The frame number from where to start the prediction.\n",
        "    n_frames (int): The number of frames to process and predict. Improtant: if 0 - the whole video is processed!\n",
        "    Returns:\n",
        "    None - Depending on the function's parameters, it may save a video with predictions or display the visualization.\n",
        "    best_model.pth contains the pretrained weights for the I3D model\n",
        "    ( weights were on the Drive&Act dataset and were fine-tuned after pretraining on Kinetics)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    trained_model_path = \"./demo_models/best_model.pth\"\n",
        "    annotation_converter_path = \"./demo_models/annotation_converter.pkl\"\n",
        "    network, annotation_converter = load_model(trained_model_path, annotation_converter_path, cuda_active=cuda_active)\n",
        "\n",
        "    #Note: out_fps must be 15 for RGB videos!\n",
        "    interate_video_and_predict(filepath_video,\n",
        "                               network=network,\n",
        "                               annotation_converter=annotation_converter,\n",
        "                               start_frame=start_frame,\n",
        "                               n_frames=n_frames, visualize=False, waitKey=100,\n",
        "                               buffer_size=32, cuda_active=cuda_active,\n",
        "                               frequency=1, video_path_out=video_path_out,\n",
        "                               out_fps=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BUVdHc0y4K33"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "# Configure logging\n",
        "def configure_logger(log_file):\n",
        "    \"\"\"\n",
        "    Configures the logger to save predictions and details to a log file.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, mode='w'),  # Save logs to file\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def log_prediction(frame_idx, top1_class, confidence):\n",
        "    \"\"\"\n",
        "    Logs the prediction for a given frame.\n",
        "\n",
        "    Args:\n",
        "        frame_idx (int): Index of the frame.\n",
        "        top1_class (str): Predicted activity.\n",
        "        confidence (float): Confidence of the prediction.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Frame {frame_idx}: Predicted Activity: {top1_class}, Confidence: {confidence:.2f}%\")\n",
        "\n",
        "def overlay_prediction(frame, prediction, confidence):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    text = f\"Prediction: {prediction} ({confidence:.2f}%)\"\n",
        "    cv2.putText(frame, text, (10, 30), font, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    return frame\n",
        "\n",
        "def sliding_window_inference(video_path, model_path, annotation_path, log_file, window_size=16, stride=1, resize=(224, 224), cuda_active=True):\n",
        "    # Configure logger\n",
        "    print(\"Configuring logger...\")\n",
        "    configure_logger(log_file)\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model, annotation_converter = load_model(model_path, annotation_path, cuda_active)\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "    print(f\"Loading video from path: {video_path}\")\n",
        "    frames = load_video_segment(video_path, start_frame=0, n_frames=0)\n",
        "    print(f\"Number of frames loaded: {len(frames)}\")\n",
        "    if not frames:\n",
        "        raise ValueError(\"No frames were loaded from the video. Check the video file or path.\")\n",
        "\n",
        "    num_frames = len(frames)\n",
        "\n",
        "    # Create a video writer for output\n",
        "    output_path = \"./annotated_output.mp4\"\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_fps = 15\n",
        "    height, width, _ = frames[0].shape\n",
        "    video_writer = cv2.VideoWriter(output_path, fourcc, out_fps, (width, height))\n",
        "\n",
        "    # Initialize a list to store predictions for each frame\n",
        "    frame_predictions = [None] * num_frames\n",
        "\n",
        "    print(\"Starting sliding window inference...\")\n",
        "\n",
        "    # Sliding window inference\n",
        "    for start in range(0, num_frames - window_size + 1, stride):\n",
        "        print(f\"Processing window: Frames {start} to {start + window_size - 1}\")\n",
        "        # Define window\n",
        "        end = start + window_size\n",
        "        window_frames = frames[start:end]\n",
        "\n",
        "        resized_window = [cv2.resize(frame, resize) for frame in window_frames]\n",
        "        # Run inference on the resized window\n",
        "        top1_class, top1_class_conf, _ = run_inference_on_video_chunk(resized_window, model, annotation_converter, cuda_active)\n",
        "        print(f\"Window Prediction: {top1_class} ({top1_class_conf * 100:.2f}%)\")\n",
        "\n",
        "        # Store predictions for all frames in the window\n",
        "        for i in range(start, end):\n",
        "            if frame_predictions[i] is None:\n",
        "                frame_predictions[i] = [(top1_class, top1_class_conf)]\n",
        "            else:\n",
        "                frame_predictions[i].append((top1_class, top1_class_conf))\n",
        "\n",
        "    print(\"Combining predictions for overlapping frames...\")\n",
        "    logs = []\n",
        "    # Combine predictions for overlapping frames\n",
        "    for i, predictions in enumerate(frame_predictions):\n",
        "        if predictions:\n",
        "            # Select the prediction with the highest confidence\n",
        "            top_prediction = max(predictions, key=lambda x: x[1])\n",
        "            top1_class, top1_class_conf = top_prediction\n",
        "\n",
        "            # # Overlay prediction on the frame\n",
        "            # frame_with_overlay = overlay_prediction(frames[i], top1_class, top1_class_conf * 100)\n",
        "\n",
        "            # # Write to output video\n",
        "            # video_writer.write(frame_with_overlay)\n",
        "\n",
        "            # # Display the frame with overlay (real-time playback)\n",
        "            # cv2.imshow(\"Video with Predictions\", frame_with_overlay)\n",
        "            # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit early\n",
        "            #     video_writer.release()\n",
        "            #     cv2.destroyAllWindows()\n",
        "            #     return\n",
        "\n",
        "            # Log predictions\n",
        "            logs.append(f\"Frame {i}: Predicted Activity: {top1_class}, Confidence: {top1_class_conf * 100:.2f}%\")\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Processed frame {i}/{num_frames}\")\n",
        "    with open(log_file, \"w\") as f:\n",
        "        f.writelines(line + \"\\n\" for line in logs)\n",
        "    # Release resources\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    logging.info(\"Inference complete. Predictions logged successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xe-ZadwU4gRZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "class Metric:\n",
        "    \"\"\"Utility class for different metrics\"\"\"\n",
        "\n",
        "    def __init__(self, ground_truth_csv, prediction_log, file_id):\n",
        "        self.ground_truth = pd.read_csv(ground_truth_csv)\n",
        "        self.predictions = self.parse_predictions(prediction_log)\n",
        "        self.segmented_predictions = self.convert_predictions_to_segments(self.predictions)\n",
        "        self.file_id = file_id\n",
        "        self.activity_classes = list(self.ground_truth['activity'].dropna().unique())\n",
        "        self.filtered_ground_truth = self.ground_truth[self.ground_truth['file_id'] == file_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_predictions(log_file):\n",
        "        predictions = []\n",
        "        with open(log_file, 'r') as file:\n",
        "            for line in file:\n",
        "                try:\n",
        "                    # Split the line to extract frame and activity information\n",
        "                    parts = line.strip().split(' - ')\n",
        "                    frame_part = parts[1].split(': ')[0]\n",
        "                    frame = int(frame_part.split()[1])\n",
        "\n",
        "                    # Extract activity and confidence\n",
        "                    activity_part = parts[1].split(': ', maxsplit=1)[1]\n",
        "                    activity = activity_part.split(', Confidence')[0].replace(\"Predicted Activity: \", \"\").strip()\n",
        "\n",
        "                    # Extract confidence if it exists\n",
        "                    confidence = 0.0  # Default confidence\n",
        "                    if \"Confidence\" in activity_part:  # Check if \"Confidence\" exists in the string\n",
        "                        confidence = float(activity_part.split('Confidence: ')[1].strip('%'))  # Extract confidence as float\n",
        "\n",
        "                    # Append to predictions list\n",
        "                    predictions.append({'frame': frame, 'activity': activity, 'confidence': confidence})\n",
        "\n",
        "                except (IndexError, ValueError) as e:\n",
        "                    print(f\"Skipping line due to error: {line.strip()} -> {e}\")\n",
        "\n",
        "        # Convert predictions to DataFrame\n",
        "        return pd.DataFrame(predictions)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_predictions_to_segments(predictions):\n",
        "        segments = []\n",
        "        current_activity = None\n",
        "        current_start = None\n",
        "\n",
        "        for _, row in predictions.iterrows():\n",
        "            frame = row['frame']\n",
        "            activity = row['activity']\n",
        "\n",
        "            # Start a new segment if the activity changes\n",
        "            if activity != current_activity:\n",
        "                if current_activity is not None:\n",
        "                    # Save the previous segment\n",
        "                    segments.append({\n",
        "                        'frame_start': current_start,\n",
        "                        'frame_end': frame - 1,\n",
        "                        'activity': current_activity\n",
        "                    })\n",
        "                # Start a new segment\n",
        "                current_activity = activity\n",
        "                current_start = frame\n",
        "\n",
        "        # Save the last segment\n",
        "        if current_activity is not None:\n",
        "            segments.append({\n",
        "                'frame_start': current_start,\n",
        "                'frame_end': predictions.iloc[-1]['frame'],\n",
        "                'activity': current_activity\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(segments)\n",
        "\n",
        "\n",
        "    def evaluate_multiclass(self):\n",
        "        all_activities = set(self.filtered_ground_truth['activity']).union(set(self.segmented_predictions['activity']))\n",
        "\n",
        "        # Initialize metrics for all known activities\n",
        "        metrics = {cls: {'tp': 0, 'fp': 0, 'fn': 0} for cls in all_activities}\n",
        "        matched_chunks = set()\n",
        "\n",
        "        for _, gt in self.filtered_ground_truth.iterrows():\n",
        "            gt_start = gt['frame_start']\n",
        "            gt_end = gt['frame_end']\n",
        "            gt_activity = gt['activity']\n",
        "            chunk_key = (gt['annotation_id'], gt['chunk_id'])\n",
        "            gt_midpoint = (gt_start + gt_end) // 2\n",
        "\n",
        "            matched_prediction = self.segmented_predictions[\n",
        "                (self.segmented_predictions['frame_start'] <= gt_midpoint) &\n",
        "                (self.segmented_predictions['frame_end'] >= gt_midpoint) &\n",
        "                (self.segmented_predictions['activity'] == gt_activity)\n",
        "            ]\n",
        "\n",
        "            if not matched_prediction.empty:\n",
        "                #  True Positive: A prediction exists for this midpoint\n",
        "                if chunk_key not in matched_chunks:\n",
        "                    metrics[gt_activity]['tp'] += 1\n",
        "                    matched_chunks.add(chunk_key)\n",
        "                else:\n",
        "                    metrics[gt_activity]['fp'] += 1\n",
        "            else:\n",
        "                # False Negative: No correct prediction found for this activity midpoint\n",
        "                metrics[gt_activity]['fn'] += 1\n",
        "\n",
        "        # Count False Positives for unmatched predictions\n",
        "        for _, pred in self.segmented_predictions.iterrows():\n",
        "            pred_activity = pred['activity']\n",
        "\n",
        "            if pred_activity in metrics and metrics[pred_activity]['tp'] > 0:\n",
        "                continue\n",
        "\n",
        "            if pred_activity not in metrics:\n",
        "                continue\n",
        "\n",
        "            metrics[pred_activity]['fp'] += 1\n",
        "\n",
        "        precision, recall = {}, {}\n",
        "        for cls in all_activities:\n",
        "            tp, fp, fn = metrics[cls]['tp'], metrics[cls]['fp'], metrics[cls]['fn']\n",
        "            precision[cls] = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0\n",
        "            recall[cls] = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
        "\n",
        "        overall_tp = sum(metrics[cls]['tp'] for cls in all_activities)\n",
        "        overall_fp = sum(metrics[cls]['fp'] for cls in all_activities)\n",
        "        overall_fn = sum(metrics[cls]['fn'] for cls in all_activities)\n",
        "\n",
        "        overall_precision = overall_tp / (overall_tp + overall_fp) * 100 if (overall_tp + overall_fp) > 0 else 0\n",
        "        overall_recall = overall_tp / (overall_tp + overall_fn) * 100 if (overall_tp + overall_fn) > 0 else 0\n",
        "\n",
        "        return precision, recall, overall_precision, overall_recall\n",
        "\n",
        "    def midpoint_hit_criteria(self):\n",
        "        \"\"\"For each ground truth activity window, if the predicted activity for the midpoint frame\n",
        "           matches the ground truth activity, we count it as a \"correct hit.\"\n",
        "\n",
        "        Returns:\n",
        "            float: correct_hits / total_instances\n",
        "        \"\"\"\n",
        "        correct_hits = 0\n",
        "        total_windows = 0\n",
        "\n",
        "        for _, row in self.ground_truth.iterrows():\n",
        "            # Get the frame range for the activity\n",
        "            start_frame = row['frame_start']\n",
        "            end_frame = row['frame_end']\n",
        "            activity = row['activity']\n",
        "            file_id = row['file_id']\n",
        "\n",
        "            # Calculate midpoint frame\n",
        "            midpoint_frame = (start_frame + end_frame) // 2\n",
        "\n",
        "            # Check prediction for the midpoint frame\n",
        "            if self.file_id == file_id:\n",
        "                if midpoint_frame in self.predictions:\n",
        "                    predicted_activity, _ = self.predictions[midpoint_frame]\n",
        "                    if predicted_activity == activity:\n",
        "                        correct_hits += 1\n",
        "\n",
        "                total_windows += 1\n",
        "\n",
        "        return correct_hits / total_windows if total_windows > 0 else 0.0\n",
        "\n",
        "    def iou(self):\n",
        "        \"\"\"\n",
        "        Calculate the Intersection over Union (IoU) metric. Here,\n",
        "        the overlapping part between the ground truth window and the predicted window is intersection and\n",
        "        the total area covered by both the ground truth and predicted windows is union.\n",
        "\n",
        "        Returns:\n",
        "            float: The average IoU score for each ground truth.\n",
        "        \"\"\"\n",
        "        iou_scores = []\n",
        "\n",
        "        for _, row in self.filtered_ground_truth.iterrows():\n",
        "            # Get the frame range for the activity\n",
        "            gt_start = row['frame_start']\n",
        "            gt_end = row['frame_end']\n",
        "            activity = row['activity']\n",
        "\n",
        "            # Find all predicted frames that match the activity\n",
        "            predicted_frames = self.predictions[\n",
        "                                    (self.predictions[\"activity\"] == activity) &\n",
        "                                    (self.predictions[\"frame\"] >= gt_start) &\n",
        "                                    (self.predictions[\"frame\"] <= gt_end)\n",
        "                                ][\"frame\"].tolist()\n",
        "\n",
        "\n",
        "            if not predicted_frames:\n",
        "                iou_scores.append(0)\n",
        "                continue\n",
        "\n",
        "            # Calculate intersection and union\n",
        "            pred_start = min(predicted_frames)\n",
        "            pred_end = max(predicted_frames)\n",
        "\n",
        "            intersection_start = max(gt_start, pred_start)\n",
        "            intersection_end = min(gt_end, pred_end)\n",
        "            intersection = max(0, intersection_end - intersection_start + 1)\n",
        "\n",
        "            union_start = min(gt_start, pred_start)\n",
        "            union_end = max(gt_end, pred_end)\n",
        "            union = max(0, union_end - union_start + 1)\n",
        "\n",
        "            iou = intersection / union if union > 0 else 0\n",
        "            iou_scores.append(iou)\n",
        "\n",
        "        return sum(iou_scores) / len(iou_scores) if iou_scores else 0.0\n",
        "\n",
        "    def evaluate(self):\n",
        "        mean_iou = self.iou()\n",
        "        precision, recall, overall_precision, overall_recall = self.evaluate_multiclass()\n",
        "\n",
        "        print(\"Precision per class (%):\", {cls: f\"{precision[cls]:.2f}%\" for cls in precision})\n",
        "        print(\"Recall per class (%):\", {cls: f\"{recall[cls]:.2f}%\" for cls in recall})\n",
        "        print(f\"Overall Precision: {overall_precision:.2f}%\")\n",
        "        print(f\"Overall Recall: {overall_recall:.2f}%\")\n",
        "        print(f\"Mean IoU: {mean_iou*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3U-T-y9vqya",
        "outputId": "3870f163-df4a-43c0-82c6-507287f0d28b"
      },
      "outputs": [],
      "source": [
        "BASE_VIDEO_PATHS=[\n",
        "    \"vp9/run1b_2018-05-23-16-19-17.kinect_color\",\n",
        "    \"vp10/run1_2018-05-24-13-14-41.kinect_color\",\n",
        "    \"vp10/run2_2018-05-24-14-08-46.kinect_color\",\n",
        "    \"vp12/run1_2018-05-24-15-44-28.kinect_color\",\n",
        "    \"vp12/run2_2018-05-24-16-21-35.kinect_color\",\n",
        "]\n",
        "\n",
        "\n",
        "MODEL_PATH=\"/content/best_model.pth\"\n",
        "ANNOTATION_PATH=\"/content/annotation_converter.pkl\"\n",
        "\n",
        "GROUND_TRUTH_CSV=\"/content/midlevel.chunks_90.csv\"\n",
        "\n",
        "WINDOW_SIZES=[8, 16 ,32]\n",
        "\n",
        "for BASE_VIDEO in BASE_VIDEO_PATHS:\n",
        "    DATA_FOLDER=\"/content/data\"\n",
        "    VIDEO_PATH=f\"{DATA_FOLDER}/{BASE_VIDEO}.mp4\"\n",
        "\n",
        "    for WINDOW_SIZE in WINDOW_SIZES:\n",
        "        LOG_FILE=f\"{DATA_FOLDER}/{BASE_VIDEO}_predictions_w{WINDOW_SIZE}.log\"\n",
        "        METRICS_OUTPUT=f\"{DATA_FOLDER}/{BASE_VIDEO}_metrics_results_w{WINDOW_SIZE}.txt\"\n",
        "\n",
        "        print(f\"Processing video: {VIDEO_PATH} with window size {WINDOW_SIZE}\")\n",
        "\n",
        "        sliding_window_inference(video_path=VIDEO_PATH,\n",
        "                             model_path=MODEL_PATH,\n",
        "                             annotation_path=ANNOTATION_PATH,\n",
        "                             log_file=LOG_FILE,\n",
        "                             window_size=WINDOW_SIZE,  # Use the user-specified window size\n",
        "                             stride=1,\n",
        "                             resize=(224, 224),\n",
        "                             cuda_active=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
